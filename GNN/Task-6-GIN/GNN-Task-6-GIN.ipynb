{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN-Task-6-GIN\n",
    "## Introduction\n",
    "在前5篇博客里面，考虑的都是node representation节点的表征，并且每个节点都有自己的特征向量代表这个节点对象的信息。而这次我们考虑的是图的表征，而不是节点的表征。我们要用GNN来学习图的特征(包括节点信息和图的结构)，要如何利用节点的特征来计算图的特征。这里我们首先考虑同构图的特征表达和学习。既然是同构图，那么就是涉及一下几个问题：\n",
    "+ 什么是同构图(isomorphisc graph)?\n",
    "+ 如何判断两个图是否同构？\n",
    "+ 如何衡量两个图的相似度？\n",
    "+ 怎么通过GIN(Graph Isomorphism Network)计算Graph Embedding?\n",
    "\n",
    "对于这几个问题,这篇博客会先回答什么是通构图，然后提及过去测试同构图和图的相似度的方法(Weisfeiler-Lehman Test and subtree kernels)，之后会回答怎么用图神经网络GIN对graph representation进行学习。\n",
    "\n",
    "## What is Isomorphic Graph\n",
    "首先什么是同构图？根据WolframMathWorld的解释: `Two graphs which contain the same number of graph vertices connected in the same way are said to be isomorphic.` 如果两个图是同构就会满足以下特点(其中第2~4点意味两个图的连接方式一样):\n",
    "1. 两个图的node个数相同\n",
    "2. 两个图的edge边数相同\n",
    "3. 两个图的node的degree序列都是一样的(两个图的node degree一一对应)\n",
    "4. 如果一个图有环，那么总能在另外一个graph找到长度相同的对应的环\n",
    "\n",
    "以下图为例，下面两个图里面的节点数(4个)和边的连接方式都是一样的(4条边，1个环)，所以下面两个图是同构图\n",
    "<img src=isomorphic-graph.png>\n",
    "\n",
    "**那么问题来了，如果图十分复杂没法用眼来观察时，怎么知道他们是同构图呢?** 这就先涉及到一个叫 WL test 和 graph kernel( Weisfeiler-Lehman Test and graph kernel)的测量方法。下面一节解释这个方法\n",
    "\n",
    "## Weisfeiler-Lehman Test and subtree kernel\n",
    "Weisfeiler-Lehman Test 的paper:https://www.jmlr.org/papers/volume12/shervashidze11a/shervashidze11a.pdf\n",
    "\n",
    "### WL test\n",
    "WL test(Weisfeiler-Lehman Test) 是一个用来判断两个图是否同构的方法。 WL Test 的一维形式，类似于图神经网络中的邻接节点聚合。WL Test步骤:\n",
    "1. 对两个图的节点进行label(一般可以把相同degree的node打上标志)\n",
    "2. 对每个node进行neighbor节点的label收集，并且排序(排序是为了确保节点表示的单射性，去除顺序带来的影响) \n",
    "3. 对每个node的节点的序列通过hashing映射到新的label。将聚合的标签散列（hash）成新标签，该过程形式化为下方的公式，\n",
    "4. 不断重复迭代地聚合节点及其邻接节点的标签\n",
    "\n",
    "$$\n",
    "L^{h}_{u} \\leftarrow \\operatorname{hash}\\left(L^{h-1}_{u} + \\sum_{v \\in \\mathcal{N}(U)} L^{h-1}_{v}\\right)\n",
    "$$\n",
    "在上方的公式中，$L^{h}_{u}$表示节点$u$的第$h$次迭代的标签，第$0$次迭代的标签为节点原始标签。\n",
    "\n",
    "在迭代过程中，发现两个图之间的节点的标签不同时，就可以确定这两个图是非同构的。需要注意的是节点标签可能的取值只能是有限个数。**WL测试不能保证对所有图都有效，特别是对于具有高度对称性的图，如链式图、完全图、环图和星图，它会判断错误。**  下面c从图a到图d是WL-test的流程图:\n",
    "<img src=WL-test.png>\n",
    "\n",
    "### WL subtree kernel\n",
    "WL-test 虽然能判断两个图是否同构但是不能测量两个图的相似度，并且有时候对高度对称的图容易判断错误。这种情况下，我们可以用WL subtree kernel方法对两个图的相似度。它的步骤是\n",
    "1. 先对两个图做迭代多次的WL-test label，即按照上面一小节的图a~d不断对图进行relabel\n",
    "2. 把两个图的多次迭代生成的所有label进行个数的统计，并将他们拼接成一个向量\n",
    "3. 把两个图的向量做inner product内积进行相似度计算，从而得到kernel值\n",
    "4. 这个kernel值越大代表两个图越相似，但是不一定是同构图。下图是subtree kernel的计算例子\n",
    "$\\phi$代表图的feature vector， $k_ {wlsubtree}$代表kernel值或两个图的相似度\n",
    "<img src=WL-subtree-kernel.png>\n",
    "\n",
    "## Graph Isomorphism Network（GIN）\n",
    "paper: https://arxiv.org/pdf/1810.00826.pdf\n",
    "### Motivaition\n",
    "根据原文, GNN的设计目前都是根据以往经验以及启发式方法和通过实验试错得到的，但是对GNN的表达能力缺乏了研究分析也缺少理论证明。而这片文章主要描述GNN的表达能力以及对其分析，另外也通过把它和WL-test 结合设计了简单的同构图网络(GIN)。 这篇文章的**重点贡献**在\n",
    "1. 理论上说明了GNN和WL-test 在图结构的识别上有同样的能力\n",
    "2. 搭建了neighbor aggregation 和 readout 函数使GNN有和WL-test相同的识别网络结构的能力\n",
    "3. 分析了那些GNN （像GCN，GraphSAGE等）识别不好的图结构\n",
    "4. 设计了和WL-test有同样的网络结构识别能力的GIN网络\n",
    "\n",
    "### How does GIN work\n",
    "+ Node Representation learning\n",
    "+ Graph Representation\n",
    "#### 简单的Readout 函数\n",
    "这篇paper也提出了readout函数用于把GNN最后一层的node representation通过把所有node的信息进行聚合从而得到一个graph的embedding。而这个readout函数一般是简单的排列不变性，和节点的特征排序无关, 比如summation， graph-level pooling。\n",
    "$$\n",
    "\\mathbf{h_ {G}} = \\textbf{READOUT}(\\{\\mathbf{h^{k}_ {v} | v \\in \\mathbf{G} }\\})\n",
    "$$\n",
    "\n",
    "#### 图同构网络GIN的构建\n",
    "\n",
    "能实现判断图同构性的图神经网络需要满足，只在两个节点自身标签一样且它们的邻接节点一样时，图神经网络将这两个节点映射到相同的表征，即映射是单射性的。**可重复集合（Multisets）指的是元素可重复的集合，元素在集合中没有顺序关系。** **一个节点的所有邻接节点是一个可重复集合，一个节点可以有重复的邻接节点，邻接节点没有顺序关系。**因此GIN模型中生成节点表征的方法遵循WL Test算法更新节点标签的过程。\n",
    "\n",
    "在GIN里面node representation的update公式是\n",
    "$$\n",
    "h_ {v}^{k} = \\text{MLP}^{k}((1+ \\epsilon^{k})h_ {v}^{(k-1)} + \\sum_ {u \\in \\mathbf{N}(v)} h_ {u}^{(k-1)})\n",
    "$$\n",
    "\n",
    "**在生成节点的表征后仍需要执行图池化（或称为图读出）操作得到图表征**，最简单的图读出操作是做求和。由于每一层的节点表征都可能是重要的，因此在图同构网络中，不同层的节点表征在求和后被拼接，其数学定义如下，\n",
    "$$\n",
    "h_{G} = \\text{CONCAT}(\\text{READOUT}\\left(\\{h_{v}^{(k)}|v\\in G\\}\\right)|k=0,1,\\cdots, K)\n",
    "$$\n",
    "**采用拼接而不是相加的原因在于不同层节点的表征属于不同的特征空间。**未做严格的证明，这样得到的图的表示与WL Subtree Kernel得到的图的表征是等价的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Coding\n",
    "**以下代码参考Stanford SNAP的 molecular的例子。** 从[官方文档](https://github.com/datawhalechina/team-learning-nlp/blob/6f8cd26d2cff4f791bab7d553b06ed652b75b854/GNN/Markdown%E7%89%88%E6%9C%AC/codes/gin_regression/gin_node.py#L8)，我们可以找到一下的GIN的node embedding, graph representation以及GINConv layer的代码. \n",
    "这里先以stanford的Open Graph Benchmark (OGB)  的原子结构图的为例。\n",
    "OBG library的AtomEncoder和BondEncoder为例。\n",
    "\n",
    "\n",
    "由于在当前的例子中，节点（原子）和边（化学键）的属性都为离散值，它们属于不同的空间，无法直接将它们融合在一起。通过嵌入（Embedding），**我们可以将节点属性和边属性分别映射到一个新的空间，在这个新的空间中，我们就可以对节点和边进行信息融合**。在`GINConv`中，`message()`函数中的`x_j + edge_attr` 操作执行了节点信息和边信息的融合。\n",
    "\n",
    "接下来，我们通过下方的代码中的`AtomEncoder`类，来分析将节点属性映射到一个新的空间是如何实现的：\n",
    "\n",
    "- `full_atom_feature_dims` 是一个链表`list`，存储了节点属性向量每一维可能取值的数量，即`X[i]` 可能的取值一共有`full_atom_feature_dims[i]`种情况，`X`为节点属性；\n",
    "- 节点属性有多少维，那么就需要有多少个嵌入函数，通过调用`torch.nn.Embedding(dim, emb_dim)`可以实例化一个嵌入函数；\n",
    "- `torch.nn.Embedding(dim, emb_dim)`，第一个参数`dim`为被嵌入数据可能取值的数量，第一个参数`emb_dim`为要映射到的空间的维度。得到的嵌入函数接受一个大于`0`小于`dim`的数，输出一个维度为`emb_dim`的向量。嵌入函数也包含可训练参数，通过对神经网络的训练，嵌入函数的输出值能够表达不同输入值之间的相似性。\n",
    "- 在`forward()`函数中，我们对不同属性值得到的不同嵌入向量进行了相加操作，实现了**将节点的的不同属性融合在一起**。\n",
    "\n",
    "`BondEncoder`类与`AtomEncoder`类是类似的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ogb.utils.features import get_atom_feature_dims, get_bond_feature_dims \n",
    "\n",
    "full_atom_feature_dims = get_atom_feature_dims()\n",
    "full_bond_feature_dims = get_bond_feature_dims()\n",
    "\n",
    "class AtomEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super(AtomEncoder, self).__init__()\n",
    "\n",
    "        self.atom_embedding_list = torch.nn.ModuleList()\n",
    "\n",
    "        for i, dim in enumerate(full_atom_feature_dims):\n",
    "            emb = torch.nn.Embedding(dim, emb_dim)\n",
    "            torch.nn.init.xavier_uniform_(emb.weight.data)\n",
    "            self.atom_embedding_list.append(emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embedding = 0\n",
    "        for i in range(x.shape[1]):\n",
    "            x_embedding += self.atom_embedding_list[i](x[:,i])\n",
    "\n",
    "        return x_embedding\n",
    "\n",
    "\n",
    "class BondEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super(BondEncoder, self).__init__()\n",
    "\n",
    "        self.bond_embedding_list = torch.nn.ModuleList()\n",
    "\n",
    "        for i, dim in enumerate(full_bond_feature_dims):\n",
    "            emb = torch.nn.Embedding(dim, emb_dim)\n",
    "            torch.nn.init.xavier_uniform_(emb.weight.data)\n",
    "            self.bond_embedding_list.append(emb)\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        bond_embedding = 0\n",
    "        for i in range(edge_attr.shape[1]):\n",
    "            bond_embedding += self.bond_embedding_list[i](edge_attr[:,i])\n",
    "\n",
    "        return bond_embedding   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 GIN 代码\n",
    "下面的GIN的架构根据ogb里面的molecular的例子的AtomEncoder, BondEncoder先对node， edge进行embedding得到节点和边的特征向量。之后基于这些特征向量的encoder搭建了GINConv卷积layer， 基于GINConv layer而搭建的GINNodeEmbedding节点信息更新的网络,以及基于GINNodeEmbedding的节点信息而计算的GINGraphEmbedding的图向量表达。 这些模块可以参考stanford的ogb的mol的源码，**不过下面的代码把源码的class的名字更改了一下**\n",
    "+ AtomEncoder and BondEncoder for mol example: https://github.com/snap-stanford/ogb/blob/master/ogb/graphproppred/mol_encoder.py\n",
    "\n",
    "+ GINConv layer: https://github.com/snap-stanford/ogb/blob/master/examples/graphproppred/mol/conv.py\n",
    "\n",
    "+  基于GINConv layer而搭建的GINNodeEmbedding： https://github.com/snap-stanford/ogb/blob/955f22515dc0e6a8231c0118f3c8760aa26c45a6/examples/graphproppred/mol/conv.py#L68\n",
    "\n",
    "+ 基于GINNodeEmbedding 而搭建的GINGraphPooling网络，输出是graph embedding： https://github.com/snap-stanford/ogb/blob/master/examples/graphproppred/mol/gnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool, GlobalAttention, Set2Set\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch.nn.functional as F\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "\n",
    "### GIN convolution along the graph structure\n",
    "class GINConv(MessagePassing):\n",
    "    def __init__(self, emb_dim):\n",
    "        '''\n",
    "            emb_dim (int): node embedding dimensionality\n",
    "        '''\n",
    "        super(GINConv, self).__init__(aggr = \"add\")\n",
    "\n",
    "        self.mlp = nn.Sequential(nn.Linear(emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU(), nn.Linear(emb_dim, emb_dim))\n",
    "        self.eps = nn.Parameter(torch.Tensor([0]))\n",
    "        self.bond_encoder = BondEncoder(emb_dim = emb_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        edge_embedding = self.bond_encoder(edge_attr) # 先将类别型边属性转换为边表征\n",
    "        out = self.mlp((1 + self.eps) *x + self.propagate(edge_index, x=x, edge_attr=edge_embedding))\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        return F.relu(x_j + edge_attr)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GNN to generate node embedding\n",
    "class GINNodeEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        node representations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers, emb_dim, drop_ratio=0.5, JK=\"last\", residual=False):\n",
    "        \"\"\"GIN Node Embedding Module\"\"\"\n",
    "\n",
    "        super(GINNodeEmbedding, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.JK = JK\n",
    "        # add residual connection or not\n",
    "        self.residual = residual\n",
    "\n",
    "        if self.num_layers < 2:\n",
    "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
    "\n",
    "        self.atom_encoder = AtomEncoder(emb_dim)\n",
    "\n",
    "        # List of GNNs\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(GINConv(emb_dim))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim))\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "        x, edge_index, edge_attr = batched_data.x, batched_data.edge_index, batched_data.edge_attr\n",
    "\n",
    "        # computing input node embedding\n",
    "        h_list = [self.atom_encoder(x)]  # 先将类别型原子属性转化为原子表征\n",
    "        for layer in range(self.num_layers):\n",
    "            h = self.convs[layer](h_list[layer], edge_index, edge_attr)\n",
    "            h = self.batch_norms[layer](h)\n",
    "            if layer == self.num_layers - 1:\n",
    "                # remove relu for the last layer\n",
    "                h = F.dropout(h, self.drop_ratio, training=self.training)\n",
    "            else:\n",
    "                h = F.dropout(F.relu(h), self.drop_ratio, training=self.training)\n",
    "\n",
    "            if self.residual:\n",
    "                h += h_list[layer]\n",
    "\n",
    "            h_list.append(h)\n",
    "\n",
    "        # Different implementations of Jk-concat\n",
    "        if self.JK == \"last\":\n",
    "            node_representation = h_list[-1]\n",
    "        elif self.JK == \"sum\":\n",
    "            node_representation = 0\n",
    "            for layer in range(self.num_layers + 1):\n",
    "                node_representation += h_list[layer]\n",
    "\n",
    "        return node_representation\n",
    "\n",
    "\n",
    "\n",
    "class GINGraphPooling(nn.Module):\n",
    "\n",
    "    def __init__(self, num_tasks=1, num_layers=5, emb_dim=300, residual=False, drop_ratio=0, JK=\"last\", graph_pooling=\"sum\"):\n",
    "        \"\"\"GIN Graph Pooling Module\n",
    "        Args:\n",
    "            num_tasks (int, optional): number of labels to be predicted. Defaults to 1 (控制了图表征的维度，dimension of graph representation).\n",
    "            num_layers (int, optional): number of GINConv layers. Defaults to 5.\n",
    "            emb_dim (int, optional): dimension of node embedding. Defaults to 300.\n",
    "            residual (bool, optional): adding residual connection or not. Defaults to False.\n",
    "            drop_ratio (float, optional): dropout rate. Defaults to 0.\n",
    "            JK (str, optional): 可选的值为\"last\"和\"sum\"。选\"last\"，只取最后一层的结点的嵌入，选\"sum\"对各层的结点的嵌入求和。Defaults to \"last\".\n",
    "            graph_pooling (str, optional): pooling method of node embedding. 可选的值为\"sum\"，\"mean\"，\"max\"，\"attention\"和\"set2set\"。 Defaults to \"sum\".\n",
    "\n",
    "        Out:\n",
    "            graph representation\n",
    "        \"\"\"\n",
    "        super(GINGraphPooling, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.JK = JK\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        if self.num_layers < 2:\n",
    "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
    "\n",
    "        self.gnn_node = GINNodeEmbedding(num_layers, emb_dim, JK=JK, drop_ratio=drop_ratio, residual=residual)\n",
    "\n",
    "        # Pooling function to generate whole-graph embeddings\n",
    "        if graph_pooling == \"sum\":\n",
    "            self.pool = global_add_pool\n",
    "        elif graph_pooling == \"mean\":\n",
    "            self.pool = global_mean_pool\n",
    "        elif graph_pooling == \"max\":\n",
    "            self.pool = global_max_pool\n",
    "        elif graph_pooling == \"attention\":\n",
    "            self.pool = GlobalAttention(gate_nn=nn.Sequential(\n",
    "                nn.Linear(emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU(), nn.Linear(emb_dim, 1)))\n",
    "        elif graph_pooling == \"set2set\":\n",
    "            self.pool = Set2Set(emb_dim, processing_steps=2)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid graph pooling type.\")\n",
    "\n",
    "        if graph_pooling == \"set2set\":\n",
    "            self.graph_pred_linear = nn.Linear(2*self.emb_dim, self.num_tasks)\n",
    "        else:\n",
    "            self.graph_pred_linear = nn.Linear(self.emb_dim, self.num_tasks)\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "        h_node = self.gnn_node(batched_data)\n",
    "\n",
    "        h_graph = self.pool(h_node, batched_data.batch)\n",
    "        output = self.graph_pred_linear(h_graph)\n",
    "\n",
    "        if self.training:\n",
    "            return output\n",
    "        else:\n",
    "            # At inference time, relu is applied to output to ensure positivity\n",
    "            # 因为预测目标的取值范围就在 (0, 50] 内\n",
    "            return torch.clamp(output, min=0, max=50)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 4/1029 [00:00<00:29, 34.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Epoch 1\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 1029/1029 [00:25<00:00, 40.35it/s]\n",
      "Iteration:   2%|▏         | 17/1029 [00:00<00:06, 160.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 1029/1029 [00:06<00:00, 164.41it/s]\n",
      "Iteration: 100%|██████████| 129/129 [00:00<00:00, 131.22it/s]\n",
      "Iteration: 100%|██████████| 129/129 [00:01<00:00, 95.27it/s]\n",
      "Iteration:   0%|          | 4/1029 [00:00<00:27, 37.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train': {'rocauc': 0.6604943642908611}, 'Validation': {'rocauc': 0.682172251616696}, 'Test': {'rocauc': 0.6643677938932772}}\n",
      "=====Epoch 2\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 1029/1029 [00:23<00:00, 43.43it/s]\n",
      "Iteration:   1%|▏         | 14/1029 [00:00<00:07, 136.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 1029/1029 [00:06<00:00, 163.67it/s]\n",
      "Iteration: 100%|██████████| 129/129 [00:00<00:00, 165.12it/s]\n",
      "Iteration: 100%|██████████| 129/129 [00:00<00:00, 165.38it/s]\n",
      "Iteration:   0%|          | 4/1029 [00:00<00:30, 34.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train': {'rocauc': 0.4996526571726294}, 'Validation': {'rocauc': 0.498015873015873}, 'Test': {'rocauc': 0.496861662063771}}\n",
      "=====Epoch 3\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 1029/1029 [00:25<00:00, 40.39it/s]\n",
      "Iteration:   2%|▏         | 16/1029 [00:00<00:06, 159.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 1029/1029 [00:06<00:00, 163.72it/s]\n",
      "Iteration: 100%|██████████| 129/129 [00:00<00:00, 165.47it/s]\n",
      "Iteration: 100%|██████████| 129/129 [00:00<00:00, 166.95it/s]\n",
      "Iteration:   0%|          | 4/1029 [00:00<00:26, 39.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train': {'rocauc': 0.5092237564450139}, 'Validation': {'rocauc': 0.5308641975308642}, 'Test': {'rocauc': 0.5}}\n",
      "=====Epoch 4\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 1029/1029 [00:25<00:00, 40.43it/s]\n",
      "Iteration:   1%|▏         | 15/1029 [00:00<00:06, 147.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 1029/1029 [00:06<00:00, 163.42it/s]\n",
      "Iteration: 100%|██████████| 129/129 [00:00<00:00, 163.16it/s]\n",
      "Iteration: 100%|██████████| 129/129 [00:00<00:00, 164.34it/s]\n",
      "Iteration:   0%|          | 4/1029 [00:00<00:27, 37.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train': {'rocauc': 0.5711309771569805}, 'Validation': {'rocauc': 0.6154636365863217}, 'Test': {'rocauc': 0.5926070414646865}}\n",
      "=====Epoch 5\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 1029/1029 [00:23<00:00, 43.58it/s]\n",
      "Iteration:   1%|▏         | 14/1029 [00:00<00:07, 140.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 1029/1029 [00:06<00:00, 163.92it/s]\n",
      "Iteration: 100%|██████████| 129/129 [00:00<00:00, 166.01it/s]\n",
      "Iteration: 100%|██████████| 129/129 [00:00<00:00, 165.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train': {'rocauc': 0.5522212973644183}, 'Validation': {'rocauc': 0.5579178301979228}, 'Test': {'rocauc': 0.6050927982386682}}\n",
      "Finished training!\n",
      "Best validation score: 0.682172251616696\n",
      "Test score: 0.6643677938932772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "### importing OGB\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "\n",
    "cls_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "reg_criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train(model, device, loader, optimizer, task_type):\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        if batch.x.shape[0] == 1 or batch.batch[-1] == 0:\n",
    "            pass\n",
    "        else:\n",
    "            pred = model(batch)\n",
    "            optimizer.zero_grad()\n",
    "            ## ignore nan targets (unlabeled) when computing training loss.\n",
    "            is_labeled = batch.y == batch.y\n",
    "            if \"classification\" in task_type: \n",
    "                loss = cls_criterion(pred.to(torch.float32)[is_labeled], batch.y.to(torch.float32)[is_labeled])\n",
    "            else:\n",
    "                loss = reg_criterion(pred.to(torch.float32)[is_labeled], batch.y.to(torch.float32)[is_labeled])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def eval(model, device, loader, evaluator):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        if batch.x.shape[0] == 1:\n",
    "            pass\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                pred = model(batch)\n",
    "\n",
    "            y_true.append(batch.y.view(pred.shape).detach().cpu())\n",
    "            y_pred.append(pred.detach().cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true, dim = 0).numpy()\n",
    "    y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
    "\n",
    "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
    "\n",
    "    return evaluator.eval(input_dict)\n",
    "\n",
    "\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.device = 0\n",
    "        self.gnn='gin'\n",
    "        self.drop_ratio = 0.5\n",
    "        self.num_layers=5\n",
    "        self.emb_dim = 300\n",
    "        self.batch_size = 32\n",
    "        self.epochs = 100\n",
    "        self.num_workers=0\n",
    "        self.dataset= \"ogbg-molhiv\"\n",
    "        self.feature=\"full\"\n",
    "        self.filename=\"\"\n",
    "\n",
    "        \n",
    "def get_terminal_args():\n",
    "    parser = argparse.ArgumentParser(description='GNN baselines on ogbgmol* data with Pytorch Geometrics')\n",
    "    parser.add_argument('--device', type=int, default=0,\n",
    "                        help='which gpu to use if any (default: 0)')\n",
    "    parser.add_argument('--gnn', type=str, default='gin-virtual',\n",
    "                        help='GNN gin, gin-virtual, or gcn, or gcn-virtual (default: gin-virtual)')\n",
    "    parser.add_argument('--drop_ratio', type=float, default=0.5,\n",
    "                        help='dropout ratio (default: 0.5)')\n",
    "    parser.add_argument('--num_layer', type=int, default=5,\n",
    "                        help='number of GNN message passing layers (default: 5)')\n",
    "    parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                        help='dimensionality of hidden units in GNNs (default: 300)')\n",
    "    parser.add_argument('--batch_size', type=int, default=32,\n",
    "                        help='input batch size for training (default: 32)')\n",
    "    parser.add_argument('--epochs', type=int, default=100,\n",
    "                        help='number of epochs to train (default: 100)')\n",
    "    parser.add_argument('--num_workers', type=int, default=0,\n",
    "                        help='number of workers (default: 0)')\n",
    "    parser.add_argument('--dataset', type=str, default=\"ogbg-molhiv\",\n",
    "                        help='dataset name (default: ogbg-molhiv)')\n",
    "\n",
    "    parser.add_argument('--feature', type=str, default=\"full\",\n",
    "                        help='full feature or simple feature')\n",
    "    parser.add_argument('--filename', type=str, default=\"\",\n",
    "                        help='filename to output result (default: )')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "def main():\n",
    "    # Training settings\n",
    "    ## if obtain settings from terminal\n",
    "    #args = get_terminal_args()\n",
    "    args = Args()\n",
    "    args.epochs = 5\n",
    "    \n",
    "\n",
    "    device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    ### automatic dataloading and splitting\n",
    "    dataset = PygGraphPropPredDataset(name = args.dataset)\n",
    "\n",
    "    if args.feature == 'full':\n",
    "        pass \n",
    "    elif args.feature == 'simple':\n",
    "        print('using simple feature')\n",
    "        # only retain the top two node/edge features\n",
    "        dataset.data.x = dataset.data.x[:,:2]\n",
    "        dataset.data.edge_attr = dataset.data.edge_attr[:,:2]\n",
    "\n",
    "    split_idx = dataset.get_idx_split()\n",
    "\n",
    "    ### automatic evaluator. takes dataset name as input\n",
    "    evaluator = Evaluator(args.dataset)\n",
    "\n",
    "    train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=args.batch_size, shuffle=True, num_workers = args.num_workers)\n",
    "    valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=args.batch_size, shuffle=False, num_workers = args.num_workers)\n",
    "    test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=args.batch_size, shuffle=False, num_workers = args.num_workers)\n",
    "    \n",
    "    if args.gnn == 'gin':\n",
    "        model = GINGraphPooling( num_tasks = dataset.num_tasks, num_layers = args.num_layers, emb_dim = args.emb_dim, drop_ratio = args.drop_ratio,).to(device)\n",
    "    else:\n",
    "        raise ValueError('Invalid GNN type')\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    valid_curve = []\n",
    "    test_curve = []\n",
    "    train_curve = []\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        print(\"=====Epoch {}\".format(epoch))\n",
    "        print('Training...')\n",
    "        train(model, device, train_loader, optimizer, dataset.task_type)\n",
    "\n",
    "        print('Evaluating...')\n",
    "        train_perf = eval(model, device, train_loader, evaluator)\n",
    "        valid_perf = eval(model, device, valid_loader, evaluator)\n",
    "        test_perf = eval(model, device, test_loader, evaluator)\n",
    "\n",
    "        print({'Train': train_perf, 'Validation': valid_perf, 'Test': test_perf})\n",
    "\n",
    "        train_curve.append(train_perf[dataset.eval_metric])\n",
    "        valid_curve.append(valid_perf[dataset.eval_metric])\n",
    "        test_curve.append(test_perf[dataset.eval_metric])\n",
    "\n",
    "    if 'classification' in dataset.task_type:\n",
    "        best_val_epoch = np.argmax(np.array(valid_curve))\n",
    "        best_train = max(train_curve)\n",
    "    else:\n",
    "        best_val_epoch = np.argmin(np.array(valid_curve))\n",
    "        best_train = min(train_curve)\n",
    "\n",
    "    print('Finished training!')\n",
    "    print('Best validation score: {}'.format(valid_curve[best_val_epoch]))\n",
    "    print('Test score: {}'.format(test_curve[best_val_epoch]))\n",
    "\n",
    "    if not args.filename == '':\n",
    "        torch.save({'Val': valid_curve[best_val_epoch], 'Test': test_curve[best_val_epoch], 'Train': train_curve[best_val_epoch], 'BestTrain': best_train}, args.filename)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(2021)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "请画出下方图片中的6号、3号和5号节点的从1层到3层的WL子树。\n",
    "\n",
    "<img src=2560px-6n-graf.svg.png>\n",
    "\n",
    "\n",
    "**6号、3号和5号节点的从1层到3层的WL子树：**\n",
    "\n",
    "<img src=assignment-wl-subtree.png>\n",
    "\n",
    "\n",
    "## Reference\n",
    "[1] https://calcworkshop.com/trees-graphs/isomorphic-graph/\n",
    "\n",
    "[2] https://mathworld.wolfram.com/IsomorphicGraphs.html\n",
    "\n",
    "[3] Stanford OGB source code: https://github.com/snap-stanford/ogb\n",
    "\n",
    "[4] Datawhale: https://github.com/datawhalechina/team-learning-nlp/blob/6f8cd26d2cff4f791bab7d553b06ed652b75b854/GNN/Markdown%E7%89%88%E6%9C%AC/8-%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9B%BE%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0.md\n",
    "\n",
    "[5] Pytorch_geometric: https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#global-pooling-layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv_v2",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
