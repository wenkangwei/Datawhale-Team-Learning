# GNN-2-Message Passing æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œ
## 1. Introduction
åœ¨å›¾ç¥ç»ç½‘ç»œé‡Œé¢ï¼Œåœ¨å¯¹æ•°æ®å’Œæ ·æœ¬ä¹‹é—´çš„å…³ç³»è¿›è¡Œå»ºæ¨¡å¾—åˆ°å›¾çš„edgeï¼Œ nodeä¹‹åï¼Œæˆ‘ä»¬éœ€è¦åœ¨å›¾é‡Œé¢æŠŠæ¯ä¸ªèŠ‚ç‚¹çš„ä¿¡æ¯æ ¹æ®å®ƒçš„neighborçš„ä¿¡æ¯è¿›è¡Œæ›´æ–°ï¼Œä»è€Œè¾¾åˆ°nodeçš„ä¿¡æ¯æ›´æ–°å’ŒèŠ‚ç‚¹ç‰¹å¾(Node Representation)çš„ç‰¹å¾è¡¨è¾¾ã€‚è€Œè¿™ä¸ªæŠŠnodeèŠ‚ç‚¹ä¿¡æ¯ç›¸äº’ä¼ é€’ä»è€Œæ›´æ–°èŠ‚ç‚¹è¡¨å¾çš„æ–¹æ³•ä¹Ÿå«Message Passingã€‚
MessagePassingæ˜¯ä¸€ç§èšåˆé‚»æ¥èŠ‚ç‚¹ä¿¡æ¯æ¥æ›´æ–°ä¸­å¿ƒèŠ‚ç‚¹ä¿¡æ¯çš„èŒƒå¼ï¼Œå®ƒå°†å·ç§¯ç®—å­æ¨å¹¿åˆ°äº†ä¸è§„åˆ™æ•°æ®é¢†åŸŸï¼Œå®ç°äº†å›¾ä¸ç¥ç»ç½‘ç»œçš„è¿æ¥ã€‚æ¶ˆæ¯ä¼ é€’èŒƒå¼å› ä¸ºç®€å•ã€å¼ºå¤§çš„ç‰¹æ€§ï¼Œäºæ˜¯è¢«äººä»¬å¹¿æ³›åœ°ä½¿ç”¨ã€‚éµå¾ªæ¶ˆæ¯ä¼ é€’èŒƒå¼çš„å›¾ç¥ç»ç½‘ç»œè¢«ç§°ä¸ºæ¶ˆæ¯ä¼ é€’å›¾ç¥ç»ç½‘ç»œã€‚

è¿™ä¸€èŠ‚é‡Œé¢æˆ‘ä»¬è®¨è®ºå’Œå®è·µ å›¾ç¥ç»ç½‘ç»œä¸€ä¸‹å‡ ç‚¹:
+ Message Passing çš„åŸç†
+ PyG (PyTorch Geometric)é‡Œé¢çš„MessagePassingç±»çš„ç†è§£å’Œæ”¹å†™
+ é€šè¿‡MessagePassing, GCNConv æ­å»ºGraph Convolution Neural network (GCN) å¹¶é€šè¿‡å®é™…çš„æ•°æ®è¿›è¡Œè®­ç»ƒ
+ å¯¹MessagePassingçš„åŸºç±»å‡½æ•°å¦‚ aggregationï¼Œ updateï¼Œ çš„methodè¿›è¡Œç†è§£å’Œä½¿ç”¨

## 2.How Message Passing works
+ **Message Passingçš„åŸºæœ¬æ€è·¯**

ä»¥å›¾ç‰‡ä¸ºä¾‹ï¼Œå¦‚æœæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯node predictionå»é¢„æµ‹node Açš„ç‰¹å¾å€¼/node representationï¼Œé‚£ä¹ˆå›¾ç‰‡é‡Œnode Aå°±æ˜¯target nodeã€‚ç„¶å MessagePassingçš„è¿‡ç¨‹å¦‚ä¸‹
   1. å›¾ä¸­é»„è‰²æ–¹æ¡†éƒ¨åˆ†å†…å®¹çš„æ˜¯ä¸€æ¬¡é‚»å±…èŠ‚ç‚¹ä¿¡æ¯ä¼ é€’åˆ°ä¸­å¿ƒèŠ‚ç‚¹çš„è¿‡ç¨‹ï¼šBèŠ‚ç‚¹çš„é‚»æ¥èŠ‚ç‚¹ï¼ˆA,Cï¼‰çš„ä¿¡æ¯ç»è¿‡å˜æ¢åèšåˆåˆ°BèŠ‚ç‚¹ï¼Œæ¥ç€BèŠ‚ç‚¹ä¿¡æ¯ä¸é‚»å±…èŠ‚ç‚¹èšåˆä¿¡æ¯ä¸€èµ·ç»è¿‡å˜æ¢å¾—åˆ°BèŠ‚ç‚¹çš„æ–°çš„èŠ‚ç‚¹ä¿¡æ¯ã€‚åŒæ—¶ï¼Œåˆ†åˆ«å¦‚çº¢è‰²å’Œç»¿è‰²æ–¹æ¡†éƒ¨åˆ†æ‰€ç¤ºï¼ŒåŒæ ·çš„è¿‡ç¨‹ï¼ŒCã€DèŠ‚ç‚¹çš„ä¿¡æ¯ä¹Ÿè¢«æ›´æ–°ã€‚å®é™…ä¸Šï¼ŒåŒæ ·çš„è¿‡ç¨‹åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šéƒ½è¿›è¡Œäº†ä¸€éï¼Œæ‰€æœ‰èŠ‚ç‚¹çš„ä¿¡æ¯éƒ½æ›´æ–°äº†ä¸€éã€‚ æ¯ä¸ªnodeçš„å€¼æ˜¯åŒæ—¶æ›´æ–°çš„
   2. æŠŠæ­¥éª¤1 çš„â€œé‚»å±…èŠ‚ç‚¹ä¿¡æ¯ä¼ é€’åˆ°ä¸­å¿ƒèŠ‚ç‚¹çš„è¿‡ç¨‹â€è¿›è¡Œå¤šæ¬¡ã€‚å¦‚å›¾ä¸­è“è‰²æ–¹æ¡†éƒ¨åˆ†æ‰€ç¤ºï¼ŒAèŠ‚ç‚¹çš„é‚»æ¥èŠ‚ç‚¹ï¼ˆB,C,Dï¼‰çš„å·²ç»å‘ç”Ÿè¿‡ä¸€æ¬¡æ›´æ–°çš„èŠ‚ç‚¹ä¿¡æ¯ï¼Œç»è¿‡å˜æ¢ã€èšåˆã€å†å˜æ¢äº§ç”Ÿäº†AèŠ‚ç‚¹ç¬¬äºŒæ¬¡æ›´æ–°çš„èŠ‚ç‚¹ä¿¡æ¯ã€‚å¤šæ¬¡æ›´æ–°åçš„èŠ‚ç‚¹ä¿¡æ¯å°±ä½œä¸ºèŠ‚ç‚¹è¡¨å¾ã€‚
   3. ä¸€å¥è¯æ€»ç»“å°±æ˜¯æ¯æ¬¡éƒ½æŠŠå›¾é‡Œé¢çš„nodeçš„ä¿¡æ¯æ ¹æ®é‚»å±…èŠ‚ç‚¹è¿›è¡Œæ›´æ–°ï¼Œå¹¶å¤šæ¬¡æŠŠå›¾çš„ä¿¡æ¯ä¸æ–­åˆ·æ–°å¾—åˆ°Node representationã€‚

![image.png](attachment:4578d9d2-08f0-42f2-b10d-9988c68bb8af.png)

+ **Message Passing GNN çš„æ³›å¼**

MessagePassingå›¾ç¥ç»ç½‘ç»œéµå¾ªä¸Šè¿°çš„â€œèšåˆé‚»æ¥èŠ‚ç‚¹ä¿¡æ¯æ¥æ›´æ–°ä¸­å¿ƒèŠ‚ç‚¹ä¿¡æ¯çš„è¿‡ç¨‹â€ï¼Œæ¥ç”ŸæˆèŠ‚ç‚¹è¡¨å¾ã€‚**Message Passing GNNçš„é€šç”¨å…¬å¼å¯ä»¥æè¿°ä¸º**
$$
\mathbf{x}_i^{(k)} = \gamma^{(k)} \left( \mathbf{x}_i^{(k-1)}, \square_{j \in \mathcal{N}(i)} \, \phi^{(k)}\left(\mathbf{x}_i^{(k-1)}, \mathbf{x}_j^{(k-1)},\mathbf{e}_{j,i}\right) \right),
$$

æ ¹æ®[å®˜æ–¹æ–‡æ¡£](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html) ä»¥åŠ[CREATING MESSAGE PASSING NETWORKS](https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html#creating-message-passing-networks), æˆ‘ä»¬å®šä¹‰
+ $\mathbf{x}^{(k-1)}_i\in\mathbb{R}^F$è¡¨ç¤ºç¥ç»ç½‘ç»œçš„$(k-1)$å±‚ä¸­èŠ‚ç‚¹$i$çš„èŠ‚ç‚¹è¡¨å¾
+ $\mathbf{e}_{j,i} \in \mathbb{R}^D$ è¡¨ç¤ºä»èŠ‚ç‚¹$j$åˆ°èŠ‚ç‚¹$i$çš„è¾¹çš„å±æ€§ä¿¡æ¯ã€‚
+ $\square$è¡¨ç¤º**å¯å¾®åˆ†**çš„ã€å…·æœ‰æ’åˆ—ä¸å˜æ€§ï¼ˆ**å‡½æ•°è¾“å‡ºç»“æœä¸è¾“å…¥å‚æ•°çš„æ’åˆ—æ— å…³**ï¼‰çš„å‡½æ•°, æ¯”å¦‚aggregation å‡½æ•°ã€‚æ¯”å¦‚sumï¼Œ mean, minç­‰å‡½æ•°å’Œè¾“å…¥çš„å‚æ•°é¡ºåºæ— å…³çš„å‡½æ•°ã€‚
+ $\gamma$ : **å¯å¾®åˆ†å¯å¯¼**çš„update å‡½æ•°ï¼Œæ¯”å¦‚MLPsï¼ˆå¤šå±‚æ„ŸçŸ¥å™¨ï¼‰
+ $\phi$: **å¯å¾®åˆ†å¯å¯¼**çš„message å‡½æ•°ï¼Œæ¯”å¦‚MLPsï¼ˆå¤šå±‚æ„ŸçŸ¥å™¨ï¼‰å’Œ linear Projectionç­‰

+ **Note:**
    1. ç¥ç»ç½‘ç»œçš„ç”ŸæˆèŠ‚ç‚¹è¡¨å¾çš„æ“ä½œç§°ä¸ºèŠ‚ç‚¹åµŒå…¥ï¼ˆNode Embeddingï¼‰ï¼ŒèŠ‚ç‚¹è¡¨å¾ä¹Ÿå¯ä»¥ç§°ä¸ºèŠ‚ç‚¹åµŒå…¥ã€‚**è¿™é‡Œè€ƒè™‘èŠ‚ç‚¹åµŒå…¥åªä»£æŒ‡ç¥ç»ç½‘ç»œç”ŸæˆèŠ‚ç‚¹è¡¨å¾çš„æ“ä½œ**ã€‚

    2. æœªç»è¿‡è®­ç»ƒçš„å›¾ç¥ç»ç½‘ç»œç”Ÿæˆçš„èŠ‚ç‚¹è¡¨å¾è¿˜ä¸æ˜¯å¥½çš„èŠ‚ç‚¹è¡¨å¾ï¼Œå¥½çš„èŠ‚ç‚¹è¡¨å¾å¯ç”¨äºè¡¡é‡èŠ‚ç‚¹ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚é€šè¿‡ç›‘ç£å­¦ä¹ å¯¹å›¾ç¥ç»ç½‘ç»œåšå¾ˆå¥½çš„è®­ç»ƒï¼Œå›¾ç¥ç»ç½‘ç»œæ‰å¯ä»¥ç”Ÿæˆå¥½çš„èŠ‚ç‚¹è¡¨å¾ã€‚æˆ‘ä»¬å°†åœ¨[ç¬¬5èŠ‚](5-åŸºäºå›¾ç¥ç»ç½‘ç»œçš„èŠ‚ç‚¹è¡¨å¾å­¦ä¹ .md)ä»‹ç»æ­¤éƒ¨åˆ†å†…å®¹ã€‚

    3. èŠ‚ç‚¹è¡¨å¾ä¸èŠ‚ç‚¹å±æ€§çš„åŒºåˆ†ï¼šéµå¾ªè¢«å¹¿æ³›ä½¿ç”¨çš„çº¦å®šï¼Œæ­¤æ¬¡ç»„é˜Ÿå­¦ä¹ æˆ‘ä»¬ä¹Ÿçº¦å®šï¼ŒèŠ‚ç‚¹å±æ€§`data.x`æ˜¯èŠ‚ç‚¹çš„ç¬¬0å±‚(GNNè¾“å…¥å±‚)èŠ‚ç‚¹è¡¨å¾ï¼Œç¬¬$h$å±‚çš„èŠ‚ç‚¹è¡¨å¾ç»è¿‡ä¸€æ¬¡çš„èŠ‚ç‚¹é—´ä¿¡æ¯ä¼ é€’äº§ç”Ÿç¬¬$h+1$å±‚çš„èŠ‚ç‚¹è¡¨å¾ã€‚ä¸è¿‡ï¼ŒèŠ‚ç‚¹å±æ€§ä¸å•æŒ‡`data.x`ï¼Œå¹¿ä¹‰ä¸Šå®ƒå°±æŒ‡èŠ‚ç‚¹çš„å±æ€§ï¼Œå¦‚èŠ‚ç‚¹çš„åº¦(in-degree, out-degree)ç­‰ã€‚



## 3. MessagePassing Class in PyTorch Geometric
### 3.1 MessagePassing çš„Base Class å‡½æ•°
Pytorch Geometric(PyG)æä¾›äº†MessagePassingåŸºç±»ï¼Œå®ƒå°è£…äº†â€œæ¶ˆæ¯ä¼ é€’â€çš„è¿è¡Œæµç¨‹ã€‚é€šè¿‡ç»§æ‰¿MessagePassingåŸºç±»ï¼Œå¯ä»¥æ–¹ä¾¿åœ°æ„é€ æ¶ˆæ¯ä¼ é€’å›¾ç¥ç»ç½‘ç»œã€‚æ„é€ ä¸€ä¸ªæœ€ç®€å•çš„æ¶ˆæ¯ä¼ é€’å›¾ç¥ç»ç½‘ç»œç±»ï¼Œæˆ‘ä»¬åªéœ€å®šä¹‰message()æ–¹æ³•ï¼ˆ ğœ™(..) ï¼‰ã€update()æ–¹æ³•ï¼ˆ ğ›¾(..) ï¼‰ï¼Œä»¥åŠä½¿ç”¨çš„æ¶ˆæ¯èšåˆæ–¹æ¡ˆï¼ˆaggr="add"ã€aggr="mean"æˆ–aggr="max"ã€‚**MessagePassing Base Classä¸­è¿™é‡Œæœ€é‡è¦çš„3ä¸ªå‡½æ•°æ˜¯ï¼š**
+ `MessagePassing.aggregate(...)`ï¼šç”¨äºå¤„ç†èšé›†åˆ°èŠ‚ç‚¹çš„ä¿¡æ¯çš„å‡½æ•°
+ `MessagePassing.message(...)`ï¼šç”¨äºæ­å»ºä¼ é€åˆ° node içš„èŠ‚ç‚¹æ¶ˆæ¯ï¼Œç›¸å¯¹äºğœ™(..)å‡½æ•°
+ `MessagePassing.update(aggr_out, ...)`: ç”¨äºæ›´æ–°èŠ‚ç‚¹çš„ä¿¡æ¯ï¼Œç›¸å¯¹äºğ›¾(..)
    
**ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸ç”¨å‡½æ•°çš„è§£é‡Š:**
+ `MessagePassing(aggr="add", flow="source_to_target", node_dim=-2)`: 
    + `aggr`: aggregation functionèšåˆå‡½æ•°çš„é€‰é¡¹, å¯ä»¥ç”¨ ("add", "mean" or "max")
    + `flow`: ä¿¡æ¯ä¼ é€’æ–¹å‘ (either "source_to_target" or "target_to_source")
    + `node_dim`ï¼šå®šä¹‰æ²¿ç€å“ªä¸ªç»´åº¦ä¼ æ’­ï¼Œé»˜è®¤å€¼ä¸º-2ï¼Œä¹Ÿå°±æ˜¯èŠ‚ç‚¹è¡¨å¾å¼ é‡ï¼ˆdata.x, Tensorï¼‰çš„å“ªä¸€ä¸ªç»´åº¦æ˜¯èŠ‚ç‚¹ç»´åº¦ã€‚èŠ‚ç‚¹è¡¨å¾å¼ é‡xå½¢çŠ¶ä¸º[num_nodes, num_features]ï¼Œå…¶ç¬¬0ç»´åº¦/columnsï¼ˆä¹Ÿæ˜¯ç¬¬-2ç»´åº¦ï¼‰æ˜¯èŠ‚ç‚¹ç»´åº¦(èŠ‚ç‚¹çš„ä¸ªæ•°)ï¼Œå…¶ç¬¬1ç»´åº¦ï¼ˆä¹Ÿæ˜¯ç¬¬-1ç»´åº¦ï¼‰æ˜¯èŠ‚ç‚¹è¡¨å¾ç»´åº¦ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥è®¾ç½®node_dim=-2ã€‚

+ `MessagePassing.propagate(edge_index, size=None, **kwargs)`: 
    + `edge_index`: ä¸€ä¸ªmatrixå­˜æ”¾æ¯æ¡edge çš„ç´¢å¼•ä¿¡æ¯(èµ·å§‹å’Œç»ˆæ­¢çš„nodeçš„index)
    + `size`: åŸºäºéå¯¹ç§°çš„é‚»æ¥çŸ©é˜µè¿›è¡Œæ¶ˆæ¯ä¼ é€’ï¼ˆå½“å›¾ä¸ºäºŒéƒ¨å›¾æ—¶ï¼‰ï¼Œéœ€è¦ä¼ é€’å‚æ•°size=(N, M)ã€‚å¦‚æœsize=None, é»˜è®¤é‚»æ¥çŸ©é˜µæ˜¯å¯¹ç§°çš„
    + `**kwargs`ï¼šå›¾çš„å…¶ä»–ç‰¹å¾

+ `MessagePassing.message(...)`ï¼š
  - é¦–å…ˆç¡®å®šè¦ç»™èŠ‚ç‚¹$i$ä¼ é€’æ¶ˆæ¯çš„è¾¹çš„é›†åˆï¼š
    - å¦‚æœ`flow="source_to_target"`ï¼Œåˆ™æ˜¯$(j,i) \in \mathcal{E}$çš„è¾¹çš„é›†åˆï¼›
    - å¦‚æœ`flow="target_to_source"`ï¼Œåˆ™æ˜¯$(i,j) \in \mathcal{E}$çš„è¾¹çš„é›†åˆã€‚
  - æ¥ç€ä¸ºå„æ¡è¾¹åˆ›å»ºè¦ä¼ é€’ç»™èŠ‚ç‚¹$i$çš„æ¶ˆæ¯ï¼Œå³å®ç°$\phi$å‡½æ•°ã€‚
  - `MessagePassing.message(...)`æ–¹æ³•å¯ä»¥æ¥æ”¶ä¼ é€’ç»™`MessagePassing.propagate(edge_index, size=None, **kwargs)`æ–¹æ³•çš„æ‰€æœ‰å‚æ•°ï¼Œæˆ‘ä»¬åœ¨`message()`æ–¹æ³•çš„å‚æ•°åˆ—è¡¨é‡Œå®šä¹‰è¦æ¥æ”¶çš„å‚æ•°ï¼Œä¾‹å¦‚æˆ‘ä»¬è¦æ¥æ”¶`x,y,z`å‚æ•°ï¼Œåˆ™æˆ‘ä»¬åº”å®šä¹‰`message(x,y,z)`æ–¹æ³•ã€‚
  - ä¼ é€’ç»™`propagate()`æ–¹æ³•çš„å‚æ•°ï¼Œå¦‚æœæ˜¯èŠ‚ç‚¹çš„å±æ€§çš„è¯ï¼Œå¯ä»¥è¢«æ‹†åˆ†æˆå±äºä¸­å¿ƒèŠ‚ç‚¹çš„éƒ¨åˆ†å’Œå±äºé‚»æ¥èŠ‚ç‚¹çš„éƒ¨åˆ†ï¼Œåªéœ€åœ¨å˜é‡ååé¢åŠ ä¸Š`_i`æˆ–`_j`ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬è‡ªå·±å®šä¹‰çš„`meassage`æ–¹æ³•åŒ…å«å‚æ•°`x_i`ï¼Œé‚£ä¹ˆé¦–å…ˆ`propagate()`æ–¹æ³•å°†èŠ‚ç‚¹è¡¨å¾æ‹†åˆ†æˆä¸­å¿ƒèŠ‚ç‚¹è¡¨å¾å’Œé‚»æ¥èŠ‚ç‚¹è¡¨å¾ï¼Œæ¥ç€`propagate()`æ–¹æ³•è°ƒç”¨`message`æ–¹æ³•å¹¶ä¼ é€’ä¸­å¿ƒèŠ‚ç‚¹è¡¨å¾ç»™å‚æ•°`x_i`ã€‚è€Œå¦‚æœæˆ‘ä»¬è‡ªå·±å®šä¹‰çš„`meassage`æ–¹æ³•åŒ…å«å‚æ•°`x_j`ï¼Œé‚£ä¹ˆ`propagate()`æ–¹æ³•ä¼šä¼ é€’é‚»æ¥èŠ‚ç‚¹è¡¨å¾ç»™å‚æ•°`x_j`ã€‚
  - æˆ‘ä»¬ç”¨$i$è¡¨ç¤ºâ€œæ¶ˆæ¯ä¼ é€’â€ä¸­çš„ä¸­å¿ƒèŠ‚ç‚¹ï¼Œç”¨$j$è¡¨ç¤ºâ€œæ¶ˆæ¯ä¼ é€’â€ä¸­çš„é‚»æ¥èŠ‚ç‚¹ã€‚
  
+ `MessagePassing.aggregate(...)`ï¼š
  - å°†ä»æºèŠ‚ç‚¹ä¼ é€’è¿‡æ¥çš„æ¶ˆæ¯èšåˆåœ¨ç›®æ ‡èŠ‚ç‚¹ä¸Šï¼Œä¸€èˆ¬å¯é€‰çš„èšåˆæ–¹å¼æœ‰`sum`, `mean`å’Œ`max`ã€‚
+ `MessagePassing.message_and_aggregate(...)`ï¼š
  - åœ¨ä¸€äº›åœºæ™¯é‡Œï¼Œé‚»æ¥èŠ‚ç‚¹ä¿¡æ¯å˜æ¢å’Œé‚»æ¥èŠ‚ç‚¹ä¿¡æ¯èšåˆè¿™ä¸¤é¡¹æ“ä½œå¯ä»¥èåˆåœ¨ä¸€èµ·ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥åœ¨æ­¤æ–¹æ³•é‡Œå®šä¹‰è¿™ä¸¤é¡¹æ“ä½œï¼Œä»è€Œè®©ç¨‹åºè¿è¡Œæ›´åŠ é«˜æ•ˆã€‚
+ `MessagePassing.update(aggr_out, ...)`: 
  - ä¸ºæ¯ä¸ªèŠ‚ç‚¹$i \in \mathcal{V}$æ›´æ–°èŠ‚ç‚¹è¡¨å¾ï¼Œå³å®ç°$\gamma$å‡½æ•°ã€‚æ­¤æ–¹æ³•ä»¥`aggregate`æ–¹æ³•çš„è¾“å‡ºä¸ºç¬¬ä¸€ä¸ªå‚æ•°ï¼Œå¹¶æ¥æ”¶æ‰€æœ‰ä¼ é€’ç»™`propagate()`æ–¹æ³•çš„å‚æ•°ã€‚



### 3.2 MessagePassing çš„Base Class å‡½æ•°
#### 3.2.1 propagate å‡½æ•°çš„è¾“å…¥
propagate å‡½æ•°çš„è¾“å…¥ æœ‰edge_index, x (node embedding matrix), ä»¥åŠå…¶ä»–è‡ªå®šä¹‰çš„è¾“å…¥å‚æ•°(degree, normä¹‹ç±»çš„)ã€‚å…¶ä¸­edge_indexçš„å‚¨å­˜å½¢å¼å¦‚ä¸‹
$$
\mathbf{Edge index}=[\begin{array}{lllll}
    [0 & 0& 1& 4&..8] \\
    [0& 1& 4& 1& ..9] \\
    \end{array}]
$$
å…¶ä¸­Edge_indexçš„shape = [2, amount of edge]. Edge_index[0]ç¬¬ä¸€è¡Œæ˜¯source nodeçš„indexï¼Œ Edge_index[1]ç¬¬äºŒè¡Œæ˜¯target nodeçš„index. 

**Note**
1. å¦‚æœedge_index ç”¨ torch tensoræ¥å‚¨å­˜ï¼Œé‚£ä¹ˆpropagateå‡½æ•°ä¼šåˆ†åˆ«è°ƒç”¨message, aggregateçš„å‡½æ•°
2. å¦‚æœedge_index ç”¨ torch_sparseçš„SparseTensorç±»æ¥å‚¨å­˜ï¼Œé‚£ä¹ˆpropagateå‡½æ•°ä¼šè°ƒç”¨message_and_aggregateçš„å‡½æ•°è€Œä¸æ˜¯ä¸¤ä¸ªå•ç‹¬çš„å‡½æ•°
3. **å½“edge_index, x(node embedding)è¾“å…¥åˆ°propagateåï¼Œå®ƒä¼šè‡ªåŠ¨é€šè¿‡ __collect__()å‡½æ•° æŠŠè¾“å…¥è§£æå¾—åˆ°ä»¥ä¸‹å‚æ•°:**
    - **å¦‚æœflow="source_to_target":**
        + **x_i**: edge_indexçš„target nodeçš„indexåˆ—è¡¨(edge_index[1])å¯¹åº”çš„node embeddingå‘é‡åˆ—è¡¨ã€‚
        æ¯”å¦‚ edge_indexçš„target nodeåˆ—è¡¨æ˜¯ edge_index[1], length = E, è€Œnode embeddingçš„ç»´åº¦ä¸ºdim, é‚£ä¹ˆ x_i =x[edge_index[1]]æ˜¯edge_index[1]æ‰€å¯¹åº”çš„embeddingåˆ—è¡¨ï¼Œ x_içš„shape= [E, dim]ã€‚
        ä¸¾ä¸ªä¾‹å­å°±æ˜¯ target node çš„ç´¢å¼•åˆ—è¡¨æ˜¯ edge_index[1] = [0, 1, 2]è€Œ E=3, dim=2, é‚£ä¹ˆ x_i = [[0.5,0.6],[0.1,0.22],[0.2,0.3]]ã€‚x_ié‡Œé¢çš„æ¯ä¸€è¡Œåˆ†åˆ«å¯¹åº”target node 0, 1,2çš„node embeddingå‘é‡
        
        + **deg_i**: edge_indexçš„target nodeçš„indexåˆ—è¡¨å¯¹åº”çš„degreeåˆ—è¡¨ã€‚è¿™ä¸ªå’Œx_iåŒç†
        + **x_j**ï¼šedge_indexçš„source nodeçš„edge_index[0]åˆ—è¡¨å¯¹åº”çš„node embeddingå‘é‡åˆ—è¡¨ã€‚
        + **deg_j**: edge_indexçš„source nodeçš„edge_index[0]åˆ—è¡¨å¯¹åº”çš„degreeåˆ—è¡¨ã€‚è¿™ä¸ªå’Œx_jåŒç†
    - **å¦‚æœflow="target_to_source" é‚£ä¹ˆæœ‰_ iåç¼€ä»£è¡¨source,  _ jåç¼€ä»£è¡¨target node**
    
4. åœ¨å¾—åˆ°target nodeçš„edge_indexå’Œ å¯¹åº”çš„source nodeçš„node embedding vectorsä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥æŠŠæ¯ä¸ªtarget nodeå¯¹åº”çš„æ‰€æœ‰node embeddingå‘é‡èšåˆä¸€èµ·å¾—åˆ°target nodeçš„ä¿¡æ¯é›†åˆç”¨äºæ­å»º messageäº†
    
#### 3.2.2 message å‡½æ•°çš„è¾“å…¥
message å‡½æ•°è¾“å…¥ä¸€èˆ¬åŒ…æ‹¬: x_i, x_j, deg_i, deg_j, edge_indexä»¥åŠå…¶ä»–è‡ªå®šä¹‰çš„å‚æ•°è¾“å…¥

#### 3.2.3 aggregate å‡½æ•°çš„è¾“å…¥
aggregate å‡½æ•°è¾“å…¥é™¤äº†æœ‰ **inputs (æ¥è‡ªmessageå‡½æ•°çš„è¾“å…¥)** å¤– ä¸€èˆ¬è¿˜åŒ…æ‹¬: inputs, x_i, x_j, deg_i, deg_j, edge_indexä»¥åŠå…¶ä»–è‡ªå®šä¹‰çš„å‚æ•°è¾“å…¥ã€‚
#### 3.2.4 message_and_aggregate å‡½æ•°çš„è¾“å…¥
message_and_aggregate å‡½æ•°è¾“å…¥ ä¸€èˆ¬è¿˜åŒ…æ‹¬: x_i, x_j, deg_i, deg_j, edge_indexä»¥åŠå…¶ä»–è‡ªå®šä¹‰çš„å‚æ•°è¾“å…¥ã€‚
#### 3.2.5 update å‡½æ•°çš„è¾“å…¥
update å‡½æ•°è¾“å…¥åŒ…æ‹¬inputsä»¥åŠå…¶ä»–è‡ªå®šä¹‰çš„å‚æ•°è¾“å…¥ã€‚



```python

```

## 4. Coding Practice
### 4.1 åŸºäº Message Passingçš„æ³›å¼(æ¡†æ¶)æ­å»ºGraph Convolution Network (GCN)

æ ¹æ®PyGçš„å®˜æ–¹æ–‡æ¡£ï¼Œ**[`GCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv)** çš„å…¬å¼æ˜¯
$$
\mathbf{x}_i^{(k)} = \sum_{j \in \mathcal{N}(i) \cup \{ i \}} \frac{1}{\sqrt{\deg(i)} \cdot \sqrt{\deg(j)}} \cdot \left( \mathbf{\Theta} \cdot \mathbf{x}_j^{(k-1)} \right),
$$

çŸ©é˜µçš„å½¢å¼æ˜¯
$$
\mathbf{X}^{(k)}  = \mathbf{D}^{-0.5}\mathbf{A}\mathbf{D}^{-0.5}\mathbf{X}^{(k-1)}\mathbf{\Theta}
$$

å…¶ä¸­ï¼Œ$\mathbf{x}_i$ çš„èŠ‚ç‚¹çš„ç‰¹å¾æ˜¯ç”±å®ƒçš„è¿‘é‚»çš„nodeçš„ä¿¡æ¯(åŒ…æ‹¬node iè‡ªå·±)è¿›è¡Œæ›´æ–°ï¼Œæ‰€ä»¥è®¡ç®—æ—¶jæ˜¯èŠ‚ç‚¹içš„é‚»å±…(åŒ…æ‹¬èŠ‚ç‚¹iæœ¬èº«)çš„å­é›†é‡Œé¢çš„nodeã€‚ é‚»æ¥èŠ‚ç‚¹çš„è¡¨å¾$\mathbf{x}_j^{(k-1)}$é¦–å…ˆé€šè¿‡ä¸æƒé‡çŸ©é˜µ$\mathbf{\Theta}$ç›¸ä¹˜è¿›è¡Œå˜æ¢ï¼Œç„¶åæŒ‰ç«¯ç‚¹çš„åº¦$\deg(i), \deg(j)$è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œæœ€åè¿›è¡Œæ±‚å’Œã€‚è¿™ä¸ªå…¬å¼å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

1. å‘é‚»æ¥çŸ©é˜µæ·»åŠ è‡ªç¯è¾¹ã€‚
1. å¯¹èŠ‚ç‚¹è¡¨å¾åšçº¿æ€§è½¬æ¢ã€‚
1. è®¡ç®—å½’ä¸€åŒ–ç³»æ•°ã€‚
1. å½’ä¸€åŒ–é‚»æ¥èŠ‚ç‚¹çš„èŠ‚ç‚¹è¡¨å¾ã€‚
1. å°†ç›¸é‚»èŠ‚ç‚¹è¡¨å¾ç›¸åŠ ï¼ˆ"æ±‚å’Œ "èšåˆï¼‰ã€‚

æ­¥éª¤1-3é€šå¸¸æ˜¯åœ¨æ¶ˆæ¯ä¼ é€’å‘ç”Ÿä¹‹å‰è®¡ç®—çš„ã€‚æ­¥éª¤4-5å¯ä»¥ä½¿ç”¨[`MessagePassing`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.message_passing.MessagePassing)åŸºç±»è½»æ¾å¤„ç†ã€‚è¯¥å±‚çš„å…¨éƒ¨å®ç°å¦‚ä¸‹æ‰€ç¤ºã€‚




```python
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops, degree
import torch


class GCNConv(MessagePassing):
    def __init__(self, in_channels, out_channels):
        super(GCNConv, self).__init__(aggr='add')  # "Add" aggregation (Step 5).
        self.lin = torch.nn.Linear(in_channels, out_channels)

    def forward(self, x, edge_index):
        # x has shape [N, in_channels]
        # edge_index has shape [2, E]

        # Step 1: Add self-loops to the adjacency matrix.
        # Adds a self-loop (i,i)âˆˆE to every node iâˆˆV in the graph given by edge_index.
        # In case the graph is weighted, self-loops will be added with edge weights denoted by fill_value.
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))

        # Step 2: Linearly transform node feature matrix.
        x = self.lin(x)

        # Step 3: Compute normalization: 1/sqrt(degree(i)) * 1/sqrt(degree(j))
        row, col = edge_index
        deg = degree(col, x.size(0), dtype=x.dtype)
        deg_inv_sqrt = deg.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]

        # Step 4-5: Start propagating messages.
        return self.propagate(edge_index, x=x, norm=norm)

    def message(self, x_j, norm):
        # x_j has shape [E, out_channels]

        # Step 4: Normalize node features.
        return norm.view(-1, 1) * x_j

    

```


```python
## download data to current directory
#! wget https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x
```


```python
from torch_geometric.datasets import Planetoid

dataset = Planetoid(root='./dataset/Cora', name='Cora')
data = dataset[0]
# GCNConv: 
#in_channels: dimension of input vector of linear layer
# out_channels: dimension of output vector of linear layer
#Note: the linear transform is performed before message passing to reduce the dimension of node representation
# After message passing, the amount of nodes doesn't change
net = GCNConv(data.num_features, 64)

# data.x: a matrix with each row representing the data in a node
# data.edge_index: matrix with shape [2, number of edges], each column representing edge from node to another node, value=index of node
h_nodes = net(data.x, data.edge_index)
print(h_nodes.shape)
```

    torch.Size([2708, 64])



```python
data.x.shape
```




    torch.Size([2708, 1433])




```python

```

### 4.2 Overwrite methods: messsage, aggregate, update


```python
from torch_geometric.datasets import Planetoid
import torch
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops, degree
from torch_sparse import SparseTensor


class GCNConv(MessagePassing):
    def __init__(self, in_channels, out_channels):
        super(GCNConv, self).__init__(aggr='add', flow='source_to_target')
        # "Add" aggregation (Step 5).
        # flow='source_to_target' è¡¨ç¤ºæ¶ˆæ¯ä»æºèŠ‚ç‚¹ä¼ æ’­åˆ°ç›®æ ‡èŠ‚ç‚¹
        self.lin = torch.nn.Linear(in_channels, out_channels)

    def forward(self, x, edge_index):
        # x has shape [N, in_channels]
        # edge_index has shape [2, E]

        # Step 1: Add self-loops to the adjacency matrix.
        print("Before self-loop:",edge_index.shape)
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))
        print("After self-loop:",edge_index.shape)
        # Step 2: Linearly transform node feature matrix.
        x = self.lin(x)

        # Step 3: Compute normalization.
        row, col = edge_index
        deg = degree(col, x.size(0), dtype=x.dtype)
        deg_inv_sqrt = deg.pow(-0.5)
        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]

        # Step 4-5: Start propagating messages.
        # Convert edge index to a sparse adjacency matrix representation, with row = from nodes, col = to nodes, value = 0 or 1 indicating if
        # two nodes are adjacent.
        adjmat = SparseTensor(row=edge_index[0], col=edge_index[1], value=torch.ones(edge_index.shape[1]))
        #print("Adjacency matrix:",adjmat)
        # æ­¤å¤„ä¼ çš„ä¸å†æ˜¯edge_idexï¼Œè€Œæ˜¯SparseTensorç±»å‹çš„Adjancency Matrix
        return self.propagate(adjmat, x=x, norm=norm, deg=deg.view((-1, 1)))

    def message(self, x_j, norm, deg_i):
        # x_j has shape [E, out_channels]
        # deg_i has shape [E, 1]
        # Step 4: Normalize node features.
        return norm.view(-1, 1) * x_j * deg_i

    def aggregate(self, inputs, index, ptr, dim_size):
        print('self.aggr:', self.aggr)
        print("`aggregate` is called")
        return super().aggregate(inputs, index, ptr=ptr, dim_size=dim_size)

    def message_and_aggregate(self, adj_t, x, norm):
        print('`message_and_aggregate` is called')
        # æ²¡æœ‰å®ç°çœŸå®çš„æ¶ˆæ¯ä¼ é€’ä¸æ¶ˆæ¯èšåˆçš„æ“ä½œ

    def update(self, inputs, deg):
        print(deg)
        return inputs


dataset = Planetoid(root='dataset/Cora', name='Cora')
data = dataset[0]

net = GCNConv(data.num_features, 64)
h_nodes = net(data.x, data.edge_index)
# print(h_nodes.shape)
```

    Before self-loop: torch.Size([2, 10556])
    After self-loop: torch.Size([2, 13264])
    Adjacency matrix: SparseTensor(row=tensor([   0,    0,    0,  ..., 2707, 2707, 2707]),
                 col=tensor([   0,  633, 1862,  ..., 1473, 2706, 2707]),
                 val=tensor([1., 1., 1.,  ..., 1., 1., 1.]),
                 size=(2708, 2708), nnz=13264, density=0.18%)
    `message_and_aggregate` is called
    tensor([[4.],
            [4.],
            [6.],
            ...,
            [2.],
            [5.],
            [5.]])


### 5. Assignment
#### 5.1 **Message Passing æœºåˆ¶æ€»ç»“**
Message Passing æ ¹æ®ä¸Šé¢è®¨è®ºçš„çš„æ¡†æ¶å…¬å¼ï¼Œåœ¨è®¾è®¡Message Passing çš„æµç¨‹å¯ä»¥å½’çº³ä¸ºä»¥ä¸‹å‡ ç‚¹:
1. å®šä¹‰å’Œé€‰å– message å‡½æ•°ï¼Œğœ™(..)ï¼Œå¹¶æ ¹æ®å›¾çš„èŠ‚ç‚¹ä¿¡æ¯çš„è¾“å…¥($x_i^{k-1}, x_j^{k-1}, e_{i,j}$) å¯¹è¾“å…¥è¿›è¡Œå˜æ¢(å¯å¯¼çš„ï¼Œæ¯”å¦‚çº¿æ€§æŠ•æ˜ è¿›è¡Œé™ç»´æˆ–ä¹˜ä¸Šç³»æ•°ä¹‹ç±»çš„)
2. å®šä¹‰å’Œé€‰å– aggregation å‡½æ•° $\square(..)$, å¯¹è½¬æ¢åçš„ä¿¡æ¯è¿›è¡Œé‚»å±…èŠ‚ç‚¹çš„ä¿¡æ¯èšåˆå¤„ç†ï¼Œ å¸¸ç”¨çš„æœ‰sum, mean, maxä¹‹ç±»çš„
3. å®šä¹‰å’Œé€‰å–update()å‡½æ•°ï¼ˆ ğ›¾(..) ï¼‰ï¼ŒæŠŠåŸæœ¬çš„èŠ‚ç‚¹ä¿¡æ¯$x_i^{k-1}$ å’Œ èšåˆåçš„é‚»å±…èŠ‚ç‚¹ä¿¡æ¯($\square(..)$ å‡½æ•°çš„è¾“å‡º)çš„ä¿¡æ¯è¿›è¡Œæ•´åˆï¼Œæ›´æ–°å½“å‰çš„èŠ‚ç‚¹ä¿¡æ¯å¾—åˆ°$x_j^{k}$ã€‚

ç”¨GCNçš„å…¬å¼ä¸¾ä¸ªæ —å­ï¼Œå°±æ˜¯ 
$$
\mathbf{x}_i^{(k)} = \sum_{j \in \mathcal{N}(i) \cup \{ i \}} \frac{1}{\sqrt{\deg(i)} \cdot \sqrt{\deg(j)}} \cdot \left( \mathbf{\Theta} \cdot \mathbf{x}_j^{(k-1)} \right),
$$

+ GCNé‡Œé¢çš„ $\frac{1}{\sqrt{\deg(i)} \cdot \sqrt{\deg(j)}} \cdot \left( \mathbf{\Theta} \cdot \mathbf{x}_j^{(k-1)} \right)$ çš„æ“ä½œï¼Œé‡Œé¢çš„$\mathbf{\Theta}$ çº¿æ€§æŠ•æ˜ å’Œç”¨degreeåšnormalizationç›¸å¯¹äºæ˜¯ ğœ™(..)å‡½æ•°çš„messageçš„æ­å»º

+ è€Œ $\sum_{j \in \mathcal{N}(i) \cup \{ i \}}$ è¿™ä¸€æ­¥ç›¸å¯¹äºæŠŠé‚»å±…èŠ‚ç‚¹(åŒ…æ‹¬èŠ‚ç‚¹è‡ªå·±)çš„ä¿¡æ¯è¿›è¡Œèšåˆ, ç›¸å¯¹äºaggregation å‡½æ•° $\square(..)$
+ GCNè¿™é‡Œå› ä¸ºåœ¨åšäº†aggregationåæ²¡æœ‰ç”¨åˆ° $x_i^{k-1}$ä¿¡æ¯ï¼Œæ‰€ä»¥update()å‡½æ•°, ğ›¾($x_i^{k-1}, \square(..)$) å¯ä»¥çœ‹æˆç›´æ¥è¾“å‡º(æˆ–è€…æ˜¯$\square()$ä¿¡æ¯èšåˆåä¹˜ä¸Š1å°±è¾“å‡º)ã€‚ğ›¾(..)å…¶å®ä¹Ÿå¯ä»¥æ›¿æ¢ä¸ºå…¶ä»–å¯å¯¼çš„çš„éçº¿æ€§å‡½æ•°æ¯”å¦‚ logisticsï¼Œ reluä¹‹ç±»çš„ã€‚
+ è‡³äºMessagePassing çš„Base Classé‡Œé¢çš„message_and_aggregate()å¯ä»¥çœ‹æˆæ˜¯ $\square(\phi(x_i^{k-1}, x_j^{k-1}, e_{i,j}))$
+ MessagePassing çš„Base Classé‡Œé¢çš„propagate()å‡½æ•°å¯ä»¥çœ‹æˆæ˜¯å¯¹ $\gamma(x_i^{k-1}, \square(\phi(...)))$ æ›´æ–°å‡½æ•°çš„å°è£…ã€‚ è¿™ä¸€ç‚¹å¯ä»¥çœ‹çœ‹å®˜æ–¹æ–‡æ¡£çš„[æºç ](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/message_passing.html#MessagePassing.propagate)



```python

```

#### 5.2 **ç”¨MessagePassing è¿™ä¸ªBaseClasså»å®ç°ä¸€ä¸ªGCN layer**
è¿™é‡Œé€æ­¥å®ç°å®ç°ä¸€ä¸ªGCNï¼Œ å…¬å¼å¦‚ä¸‹:

$$
\mathbf{x}_i^{(k)} = \sum_{j \in \mathcal{N}(i) \cup \{ i \}} \frac{1}{\sqrt{\deg(i)} \cdot \sqrt{\deg(j)}} \cdot \left( \mathbf{\Theta} \cdot \mathbf{x}_j^{(k-1)} \right),
$$

è¿™é‡Œä¸€äº›å‡½æ•°å®šä¹‰å¦‚ä¸‹ï¼š
+ $\phi(..)$: messageå‡½æ•°GCNä¸€æ ·éƒ½æ˜¯linear projectionä¹‹åç”¨degreeè¿›è¡Œnormalization
+ $\square(..)$ : aggregate å‡½æ•°ç”¨ add
+ $\gamma(..)$: update å‡½æ•°æ˜¯ç›´æ¥å°†aggregateåçš„ç»“æœè¾“å‡º


#### 5.2.1 è¦†å†™messageå‡½æ•°
è¦æ±‚è¯¥å‡½æ•°æ¥æ”¶æ¶ˆæ¯ä¼ é€’æºèŠ‚ç‚¹å±æ€§xã€ç›®æ ‡èŠ‚ç‚¹åº¦d


```python
from torch_geometric.datasets import Planetoid
import torch
from torch import nn, Tensor
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops, degree
from torch_sparse import SparseTensor, matmul


class GCNConv(MessagePassing):
    def __init__(self, in_channels, out_channels):
        super(GCNConv, self).__init__(aggr='add', flow='source_to_target')
        # "Add" aggregation (Step 5).
        # flow='source_to_target' è¡¨ç¤ºæ¶ˆæ¯ä»æºèŠ‚ç‚¹ä¼ æ’­åˆ°ç›®æ ‡èŠ‚ç‚¹
        self.lin = torch.nn.Linear(in_channels, out_channels)
        self.lin2 = torch.nn.Linear(out_channels, out_channels)
        self.relu = torch.nn.ReLU()

        
    def propagate(self, edge_index, size=None, **kwargs):
        # I just copy the source copy from PyG website
        r"""The initial call to start propagating messages.

        Args:
            edge_index (Tensor or SparseTensor): A :obj:`torch.LongTensor` or a
                :obj:`torch_sparse.SparseTensor` that defines the underlying
                graph connectivity/message passing flow.
                :obj:`edge_index` holds the indices of a general (sparse)
                assignment matrix of shape :obj:`[N, M]`.
                If :obj:`edge_index` is of type :obj:`torch.LongTensor`, its
                shape must be defined as :obj:`[2, num_messages]`, where
                messages from nodes in :obj:`edge_index[0]` are sent to
                nodes in :obj:`edge_index[1]`
                (in case :obj:`flow="source_to_target"`).
                If :obj:`edge_index` is of type
                :obj:`torch_sparse.SparseTensor`, its sparse indices
                :obj:`(row, col)` should relate to :obj:`row = edge_index[1]`
                and :obj:`col = edge_index[0]`.
                The major difference between both formats is that we need to
                input the *transposed* sparse adjacency matrix into
                :func:`propagate`.
            size (tuple, optional): The size :obj:`(N, M)` of the assignment
                matrix in case :obj:`edge_index` is a :obj:`LongTensor`.
                If set to :obj:`None`, the size will be automatically inferred
                and assumed to be quadratic.
                This argument is ignored in case :obj:`edge_index` is a
                :obj:`torch_sparse.SparseTensor`. (default: :obj:`None`)
            **kwargs: Any additional data which is needed to construct and
                aggregate messages, and to update node embeddings.
        """
        size = self.__check_input__(edge_index, size)

        # Run "fused" message and aggregation (if applicable).
        if (isinstance(edge_index, SparseTensor) and self.fuse
                and not self.__explain__):
            coll_dict = self.__collect__(self.__fused_user_args__, edge_index,
                                         size, kwargs)
            print("Using self-defined message-passing")
            msg_aggr_kwargs = self.inspector.distribute(
                'message_and_aggregate', coll_dict)
            out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs)

            update_kwargs = self.inspector.distribute('update', coll_dict)
            return self.update(out, **update_kwargs)

        # Otherwise, run both functions in separation.
        elif isinstance(edge_index, Tensor) or not self.fuse:
            coll_dict = self.__collect__(self.__user_args__, edge_index, size,
                                         kwargs)

            msg_kwargs = self.inspector.distribute('message', coll_dict)
            #print("Message kwargs: ",msg_kwargs)
            out = self.message(**msg_kwargs)

            # For `GNNExplainer`, we require a separate message and aggregate
            # procedure since this allows us to inject the `edge_mask` into the
            # message passing computation scheme.
            if self.__explain__:
                edge_mask = self.__edge_mask__.sigmoid()
                # Some ops add self-loops to `edge_index`. We need to do the
                # same for `edge_mask` (but do not train those).
                if out.size(self.node_dim) != edge_mask.size(0):
                    loop = edge_mask.new_ones(size[0])
                    edge_mask = torch.cat([edge_mask, loop], dim=0)
                assert out.size(self.node_dim) == edge_mask.size(0)
                out = out * edge_mask.view([-1] + [1] * (out.dim() - 1))

            aggr_kwargs = self.inspector.distribute('aggregate', coll_dict)
            out = self.aggregate(out, **aggr_kwargs)

            update_kwargs = self.inspector.distribute('update', coll_dict)
            return self.update(out, **update_kwargs)
        
        
    def forward(self, x, edge_index):
        # x has shape [N, in_channels]
        # edge_index has shape [2, E]

        # Step 1: Add self-loops to the adjacency matrix.
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))

        # Step 2: Linearly transform node feature matrix.
        x = self.lin(x)

        # Compute degree.
        row, col = edge_index
        deg = degree(col, x.size(0), dtype=x.dtype)
        
        return self.propagate(edge_index, x=x, deg=deg.view((-1, 1)))
        


    def message(self, x_j, deg_i,deg_j):
        # Accoding to __collect__ function 
        # in https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/nn/conv/message_passing.py
        # when flow = source_to_target
        # i= 1, j=0, edge_index_i = edge_index[1] = target, so 
        # deg_i is degree of target node,  and x_i is target node data
        # deg_j is degree of source node and  x_j is source 
        # x_j has shape [E, out_channels]
        # deg_i has shape [E, 1]
        
        
        # Step 3: Normalize node features.
        print("--message is called--")
        print("x_j: ",x_j.shape)
        print("degree: ", deg_i.shape)
        print("degree: ",deg_j.shape)
        print()
        # check if degrees of source nodes and degrees of target nodes are equal
        print(torch.eq(deg_i, deg_j).all())
        # compute normalization
        deg_i = deg_i.pow(-0.5)
        deg_j = deg_j.pow(-0.5)
        norm = deg_i * deg_j
        
        return norm.view(-1, 1) * x_j

dataset = Planetoid(root='dataset/Cora', name='Cora')
data = dataset[0]

net = GCNConv(data.num_features, 64)
h_nodes = net(data.x, data.edge_index)
print("H_nodes: ", h_nodes.shape)
h_nodes
```

    --message is called--
    x_j:  torch.Size([13264, 64])
    degree:  torch.Size([13264, 1])
    degree:  torch.Size([13264, 1])
    
    tensor(False)
    H_nodes:  torch.Size([2708, 64])





    tensor([[-0.0336, -0.0263, -0.0141,  ..., -0.0157, -0.0207,  0.0233],
            [-0.0204, -0.0698, -0.0737,  ..., -0.0233,  0.0268, -0.0347],
            [-0.0437, -0.0602, -0.0162,  ...,  0.0243,  0.0348, -0.0054],
            ...,
            [-0.0067, -0.0016, -0.0004,  ...,  0.0237, -0.0289,  0.0044],
            [ 0.0061,  0.0198, -0.0076,  ...,  0.0065,  0.0373, -0.0187],
            [ 0.0080,  0.0146, -0.0173,  ..., -0.0250,  0.0205,  0.0163]],
           grad_fn=<ScatterAddBackward>)




```python

```

#### 5.2.2 åœ¨ç¬¬ä¸€ä¸ªç±»çš„åŸºç¡€ä¸Šï¼Œå†è¦†å†™aggregateå‡½æ•°
è¦æ±‚ä¸èƒ½è°ƒç”¨superç±»çš„aggregateå‡½æ•°ï¼Œå¹¶ä¸”ä¸èƒ½ç›´æ¥å¤åˆ¶superç±»çš„aggregateå‡½æ•°å†…å®¹ã€‚


```python
from torch_geometric.datasets import Planetoid
import torch
from torch import nn, Tensor
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops, degree
from torch_sparse import SparseTensor, matmul


class GCNConv(MessagePassing):
    def __init__(self, in_channels, out_channels):
        super(GCNConv, self).__init__(aggr='add', flow='source_to_target')
        # "Add" aggregation (Step 5).
        # flow='source_to_target' è¡¨ç¤ºæ¶ˆæ¯ä»æºèŠ‚ç‚¹ä¼ æ’­åˆ°ç›®æ ‡èŠ‚ç‚¹
        self.lin = torch.nn.Linear(in_channels, out_channels)
        self.lin2 = torch.nn.Linear(out_channels, out_channels)
        self.relu = torch.nn.ReLU()

        
    def propagate(self, edge_index, size=None, **kwargs):
        # I just copy the source copy from PyG website
        r"""The initial call to start propagating messages.

        Args:
            edge_index (Tensor or SparseTensor): A :obj:`torch.LongTensor` or a
                :obj:`torch_sparse.SparseTensor` that defines the underlying
                graph connectivity/message passing flow.
                :obj:`edge_index` holds the indices of a general (sparse)
                assignment matrix of shape :obj:`[N, M]`.
                If :obj:`edge_index` is of type :obj:`torch.LongTensor`, its
                shape must be defined as :obj:`[2, num_messages]`, where
                messages from nodes in :obj:`edge_index[0]` are sent to
                nodes in :obj:`edge_index[1]`
                (in case :obj:`flow="source_to_target"`).
                If :obj:`edge_index` is of type
                :obj:`torch_sparse.SparseTensor`, its sparse indices
                :obj:`(row, col)` should relate to :obj:`row = edge_index[1]`
                and :obj:`col = edge_index[0]`.
                The major difference between both formats is that we need to
                input the *transposed* sparse adjacency matrix into
                :func:`propagate`.
            size (tuple, optional): The size :obj:`(N, M)` of the assignment
                matrix in case :obj:`edge_index` is a :obj:`LongTensor`.
                If set to :obj:`None`, the size will be automatically inferred
                and assumed to be quadratic.
                This argument is ignored in case :obj:`edge_index` is a
                :obj:`torch_sparse.SparseTensor`. (default: :obj:`None`)
            **kwargs: Any additional data which is needed to construct and
                aggregate messages, and to update node embeddings.
        """
        size = self.__check_input__(edge_index, size)

        # Run "fused" message and aggregation (if applicable).
        if (isinstance(edge_index, SparseTensor) and self.fuse
                and not self.__explain__):
            coll_dict = self.__collect__(self.__fused_user_args__, edge_index,
                                         size, kwargs)
            print("Using self-defined message-passing")
            msg_aggr_kwargs = self.inspector.distribute(
                'message_and_aggregate', coll_dict)
            out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs)

            update_kwargs = self.inspector.distribute('update', coll_dict)
            return self.update(out, **update_kwargs)

        # Otherwise, run both functions in separation.
        elif isinstance(edge_index, Tensor) or not self.fuse:
            coll_dict = self.__collect__(self.__user_args__, edge_index, size,
                                         kwargs)

            msg_kwargs = self.inspector.distribute('message', coll_dict)
            #print("Message kwargs: ",msg_kwargs)
            out = self.message(**msg_kwargs)

            # For `GNNExplainer`, we require a separate message and aggregate
            # procedure since this allows us to inject the `edge_mask` into the
            # message passing computation scheme.
            if self.__explain__:
                edge_mask = self.__edge_mask__.sigmoid()
                # Some ops add self-loops to `edge_index`. We need to do the
                # same for `edge_mask` (but do not train those).
                if out.size(self.node_dim) != edge_mask.size(0):
                    loop = edge_mask.new_ones(size[0])
                    edge_mask = torch.cat([edge_mask, loop], dim=0)
                assert out.size(self.node_dim) == edge_mask.size(0)
                out = out * edge_mask.view([-1] + [1] * (out.dim() - 1))

            aggr_kwargs = self.inspector.distribute('aggregate', coll_dict)
            out = self.aggregate(out, **aggr_kwargs)

            update_kwargs = self.inspector.distribute('update', coll_dict)
            return self.update(out, **update_kwargs)
        
        
    def forward(self, x, edge_index):
        # x has shape [N, in_channels]
        # edge_index has shape [2, E]

        # Step 1: Add self-loops to the adjacency matrix.
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))

        # Step 2: Linearly transform node feature matrix.
        x = self.lin(x)

        # Compute degree.
        row, col = edge_index
        deg = degree(col, x.size(0), dtype=x.dtype)
        
        return self.propagate(edge_index, x=x, deg=deg.view((-1, 1)))
        


    def message(self, x_j, deg_i,deg_j):
        # Accoding to __collect__ function 
        # in https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/nn/conv/message_passing.py
        # when flow = source_to_target
        # i= 1, j=0, edge_index_i = edge_index[1] = target, so 
        # deg_i is degree of target node,  and x_i is target node data
        # deg_j is degree of source node and  x_j is source 
        # x_j has shape [E, out_channels]
        # deg_i has shape [E, 1]
        
        
        # Step 3: Normalize node features.
        print("--message is called--")
        print("x_j: ",x_j.shape)
        print("degree: ", deg_i.shape)
        print("degree: ",deg_j.shape)
        print()
        # check if degrees of source nodes and degrees of target nodes are equal
        print(torch.eq(deg_i, deg_j).all())
        # compute normalization
        deg_i = deg_i.pow(-0.5)
        deg_j = deg_j.pow(-0.5)
        norm = deg_i * deg_j
        
        return norm.view(-1, 1) * x_j
    
    def aggregate(self, inputs, index, ptr, dim_size):
        #from __collect__() function we know that
        # when flow = source_to_target
        # out['index'] = out['edge_index_i']  -> input index = edge_index[i] = edge_index[1] = index of target node
        # inputs: embedding vectors of source nodes
        # inputs: the outputs from message function, the normalized source node embeding with shape [E, dim of embedding]
        
        print("--aggregate` is called--")
        print('self.aggr:', self.aggr)
        print('ptr: ', ptr)
        print('dim_size: ',dim_size)
        print("inputs: ", inputs.shape)
        print("index: ",index.shape, len(index.unique()))
        print()
        uni_idx = index.unique()
        uni_idx.sort()
        
        res= []
        # find all unique target node index
        # for each target node, aggregate(sum or mean ) the information from source node to the target node
        # and obtain target node embedding
        for i in uni_idx:
            # i is the index of target node
            neighbors = inputs[index == i]
            # aggregate along different vectors of different nodes
            if self.aggr=="mean":
                agg_res = neighbors.mean(axis=0)
            else:
                agg_res = neighbors.sum(axis=0)
            res.append(agg_res)
        res = torch.stack(res)
        return res 
    
dataset = Planetoid(root='dataset/Cora', name='Cora')
data = dataset[0]

net = GCNConv(data.num_features, 64)
h_nodes = net(data.x, data.edge_index)
print("H_nodes: ", h_nodes.shape)
h_nodes
```

    --message is called--
    x_j:  torch.Size([13264, 64])
    degree:  torch.Size([13264, 1])
    degree:  torch.Size([13264, 1])
    
    tensor(False)
    --aggregate` is called--
    self.aggr: add
    ptr:  None
    dim_size:  2708
    inputs:  torch.Size([13264, 64])
    index:  torch.Size([13264]) 2708
    
    H_nodes:  torch.Size([2708, 64])





    tensor([[-0.0141,  0.0188,  0.0067,  ..., -0.0314,  0.0296, -0.0301],
            [ 0.0056, -0.0510,  0.0796,  ..., -0.0591,  0.0362,  0.0113],
            [-0.0034,  0.0314,  0.0107,  ..., -0.0433,  0.0407,  0.0185],
            ...,
            [ 0.0280,  0.0239,  0.0307,  ..., -0.0530, -0.0522,  0.0293],
            [-0.0094,  0.0380, -0.0108,  ..., -0.0115,  0.0182, -0.0060],
            [-0.0058, -0.0127, -0.0221,  ..., -0.0027,  0.0008, -0.0052]],
           grad_fn=<StackBackward>)




```python

```

#### 5.2.3 åœ¨ç¬¬äºŒä¸ªç±»çš„åŸºç¡€ä¸Šï¼Œå†è¦†å†™updateå‡½æ•°
è¦æ±‚å¯¹èŠ‚ç‚¹ä¿¡æ¯åšä¸€å±‚çº¿æ€§å˜æ¢ã€‚


```python
from torch_geometric.datasets import Planetoid
import torch
from torch import nn, Tensor
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops, degree
from torch_sparse import SparseTensor, matmul


class GCNConv(MessagePassing):
    def __init__(self, in_channels, out_channels):
        super(GCNConv, self).__init__(aggr='add', flow='source_to_target')
        # "Add" aggregation (Step 5).
        # flow='source_to_target' è¡¨ç¤ºæ¶ˆæ¯ä»æºèŠ‚ç‚¹ä¼ æ’­åˆ°ç›®æ ‡èŠ‚ç‚¹
        self.lin = torch.nn.Linear(in_channels, out_channels)
        self.lin2 = torch.nn.Linear(out_channels, out_channels)
        self.relu = torch.nn.ReLU()

        
    def propagate(self, edge_index, size=None, **kwargs):
        # I just copy the source copy from PyG website
        r"""The initial call to start propagating messages.

        Args:
            edge_index (Tensor or SparseTensor): A :obj:`torch.LongTensor` or a
                :obj:`torch_sparse.SparseTensor` that defines the underlying
                graph connectivity/message passing flow.
                :obj:`edge_index` holds the indices of a general (sparse)
                assignment matrix of shape :obj:`[N, M]`.
                If :obj:`edge_index` is of type :obj:`torch.LongTensor`, its
                shape must be defined as :obj:`[2, num_messages]`, where
                messages from nodes in :obj:`edge_index[0]` are sent to
                nodes in :obj:`edge_index[1]`
                (in case :obj:`flow="source_to_target"`).
                If :obj:`edge_index` is of type
                :obj:`torch_sparse.SparseTensor`, its sparse indices
                :obj:`(row, col)` should relate to :obj:`row = edge_index[1]`
                and :obj:`col = edge_index[0]`.
                The major difference between both formats is that we need to
                input the *transposed* sparse adjacency matrix into
                :func:`propagate`.
            size (tuple, optional): The size :obj:`(N, M)` of the assignment
                matrix in case :obj:`edge_index` is a :obj:`LongTensor`.
                If set to :obj:`None`, the size will be automatically inferred
                and assumed to be quadratic.
                This argument is ignored in case :obj:`edge_index` is a
                :obj:`torch_sparse.SparseTensor`. (default: :obj:`None`)
            **kwargs: Any additional data which is needed to construct and
                aggregate messages, and to update node embeddings.
        """
        size = self.__check_input__(edge_index, size)

        # Run "fused" message and aggregation (if applicable).
        if (isinstance(edge_index, SparseTensor) and self.fuse
                and not self.__explain__):
            coll_dict = self.__collect__(self.__fused_user_args__, edge_index,
                                         size, kwargs)
            print("Using self-defined message-passing")
            msg_aggr_kwargs = self.inspector.distribute(
                'message_and_aggregate', coll_dict)
            out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs)

            update_kwargs = self.inspector.distribute('update', coll_dict)
            return self.update(out, **update_kwargs)

        # Otherwise, run both functions in separation.
        elif isinstance(edge_index, Tensor) or not self.fuse:
            coll_dict = self.__collect__(self.__user_args__, edge_index, size,
                                         kwargs)

            msg_kwargs = self.inspector.distribute('message', coll_dict)
            #print("Message kwargs: ",msg_kwargs)
            out = self.message(**msg_kwargs)

            # For `GNNExplainer`, we require a separate message and aggregate
            # procedure since this allows us to inject the `edge_mask` into the
            # message passing computation scheme.
            if self.__explain__:
                edge_mask = self.__edge_mask__.sigmoid()
                # Some ops add self-loops to `edge_index`. We need to do the
                # same for `edge_mask` (but do not train those).
                if out.size(self.node_dim) != edge_mask.size(0):
                    loop = edge_mask.new_ones(size[0])
                    edge_mask = torch.cat([edge_mask, loop], dim=0)
                assert out.size(self.node_dim) == edge_mask.size(0)
                out = out * edge_mask.view([-1] + [1] * (out.dim() - 1))

            aggr_kwargs = self.inspector.distribute('aggregate', coll_dict)
            out = self.aggregate(out, **aggr_kwargs)

            update_kwargs = self.inspector.distribute('update', coll_dict)
            return self.update(out, **update_kwargs)
        
        
    def forward(self, x, edge_index):
        # x has shape [N, in_channels]
        # edge_index has shape [2, E]

        # Step 1: Add self-loops to the adjacency matrix.
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))

        # Step 2: Linearly transform node feature matrix.
        x = self.lin(x)

        # Compute degree.
        row, col = edge_index
        deg = degree(col, x.size(0), dtype=x.dtype)
        
        return self.propagate(edge_index, x=x, deg=deg.view((-1, 1)))
        


    def message(self, x_j, deg_i,deg_j):
        # Accoding to __collect__ function 
        # in https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/nn/conv/message_passing.py
        # when flow = source_to_target
        # i= 1, j=0, edge_index_i = edge_index[1] = target, so 
        # deg_i is degree of target node,  and x_i is target node data
        # deg_j is degree of source node and  x_j is source 
        # x_j has shape [E, out_channels]
        # deg_i has shape [E, 1]
        
        
        # Step 3: Normalize node features.
        print("--message is called--")
        print("x_j: ",x_j.shape)
        print("degree: ", deg_i.shape)
        print("degree: ",deg_j.shape)
        print()
        # check if degrees of source nodes and degrees of target nodes are equal
        print(torch.eq(deg_i, deg_j).all())
        # compute normalization
        deg_i = deg_i.pow(-0.5)
        deg_j = deg_j.pow(-0.5)
        norm = deg_i * deg_j
        
        return norm.view(-1, 1) * x_j
    
    def aggregate(self, inputs, index, ptr, dim_size):
        #from __collect__() function we know that
        # when flow = source_to_target
        # out['index'] = out['edge_index_i']  -> input index = edge_index[i] = edge_index[1] = index of target node
        # inputs: embedding vectors of source nodes
        # inputs: the outputs from message function, the normalized source node embeding with shape [E, dim of embedding]
        
        print("--aggregate` is called--")
        print('self.aggr:', self.aggr)
        print('ptr: ', ptr)
        print('dim_size: ',dim_size)
        print("inputs: ", inputs.shape)
        print("index: ",index.shape, len(index.unique()))
        print()
        uni_idx = index.unique()
        uni_idx.sort()
        
        res= []
        # find all unique target node index
        # for each target node, aggregate(sum or mean ) the information from source node to the target node
        # and obtain target node embedding
        for i in uni_idx:
            # i is the index of target node
            neighbors = inputs[index == i]
            # aggregate along different vectors of different nodes
            if self.aggr=="mean":
                agg_res = neighbors.mean(axis=0)
            else:
                agg_res = neighbors.sum(axis=0)
            res.append(agg_res)
        res = torch.stack(res)
        return res 
    
    def update(self,inputs, deg ):
        print("--update func is called--")
        return self.lin2(inputs)

dataset = Planetoid(root='dataset/Cora', name='Cora')
data = dataset[0]

net = GCNConv(data.num_features, 64)
h_nodes = net(data.x, data.edge_index)
print("H_nodes: ", h_nodes.shape)
h_nodes
```

    --message is called--
    x_j:  torch.Size([13264, 64])
    degree:  torch.Size([13264, 1])
    degree:  torch.Size([13264, 1])
    tensor(False)
    --aggregate` is called--
    self.aggr: add
    ptr:  None
    dim_size:  2708
    inputs:  torch.Size([13264, 64])
    index:  torch.Size([13264]) 2708
    
    --update func is called--
    H_nodes:  torch.Size([2708, 64])





    tensor([[-0.0139, -0.0065,  0.1316,  ...,  0.0401, -0.1439, -0.0718],
            [-0.0333, -0.0545,  0.1637,  ..., -0.0098, -0.1503, -0.0837],
            [-0.0245, -0.0277,  0.1248,  ...,  0.0264, -0.1423, -0.0829],
            ...,
            [-0.0678, -0.0061,  0.1510,  ...,  0.0332, -0.1420, -0.0876],
            [-0.0289, -0.0100,  0.1211,  ...,  0.0339, -0.1905, -0.0764],
            [-0.0255, -0.0036,  0.1290,  ...,  0.0366, -0.1623, -0.0631]],
           grad_fn=<AddmmBackward>)




```python

```

#### 5.2.4 åœ¨ç¬¬ä¸‰ä¸ªç±»çš„åŸºç¡€ä¸Šï¼Œå†è¦†å†™message_and_aggregateå‡½æ•°
è¦æ±‚åœ¨è¿™ä¸€ä¸ªå‡½æ•°ä¸­å®ç°å‰é¢messageå‡½æ•°å’Œaggregateå‡½æ•°çš„åŠŸèƒ½ã€‚


```python
from torch_geometric.datasets import Planetoid
import torch
from torch import nn, Tensor
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops, degree
from torch_sparse import SparseTensor, matmul


class GCNConv(MessagePassing):
    def __init__(self, in_channels, out_channels):
        super(GCNConv, self).__init__(aggr='add', flow='source_to_target')
        # "Add" aggregation (Step 5).
        # flow='source_to_target' è¡¨ç¤ºæ¶ˆæ¯ä»æºèŠ‚ç‚¹ä¼ æ’­åˆ°ç›®æ ‡èŠ‚ç‚¹
        self.lin = torch.nn.Linear(in_channels, out_channels)
        self.lin2 = torch.nn.Linear(out_channels, out_channels)
        self.relu = torch.nn.ReLU()

        
    def propagate(self, edge_index, size=None, **kwargs):
        # I just copy the source copy from PyG website
        r"""The initial call to start propagating messages.

        Args:
            edge_index (Tensor or SparseTensor): A :obj:`torch.LongTensor` or a
                :obj:`torch_sparse.SparseTensor` that defines the underlying
                graph connectivity/message passing flow.
                :obj:`edge_index` holds the indices of a general (sparse)
                assignment matrix of shape :obj:`[N, M]`.
                If :obj:`edge_index` is of type :obj:`torch.LongTensor`, its
                shape must be defined as :obj:`[2, num_messages]`, where
                messages from nodes in :obj:`edge_index[0]` are sent to
                nodes in :obj:`edge_index[1]`
                (in case :obj:`flow="source_to_target"`).
                If :obj:`edge_index` is of type
                :obj:`torch_sparse.SparseTensor`, its sparse indices
                :obj:`(row, col)` should relate to :obj:`row = edge_index[1]`
                and :obj:`col = edge_index[0]`.
                The major difference between both formats is that we need to
                input the *transposed* sparse adjacency matrix into
                :func:`propagate`.
            size (tuple, optional): The size :obj:`(N, M)` of the assignment
                matrix in case :obj:`edge_index` is a :obj:`LongTensor`.
                If set to :obj:`None`, the size will be automatically inferred
                and assumed to be quadratic.
                This argument is ignored in case :obj:`edge_index` is a
                :obj:`torch_sparse.SparseTensor`. (default: :obj:`None`)
            **kwargs: Any additional data which is needed to construct and
                aggregate messages, and to update node embeddings.
        """
        size = self.__check_input__(edge_index, size)

        # Run "fused" message and aggregation (if applicable).
        if (isinstance(edge_index, SparseTensor) and self.fuse
                and not self.__explain__):
            coll_dict = self.__collect__(self.__fused_user_args__, edge_index,
                                         size, kwargs)
            #print("Using self-defined message-passing")
            msg_aggr_kwargs = self.inspector.distribute(
                'message_and_aggregate', coll_dict)
            out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs)

            update_kwargs = self.inspector.distribute('update', coll_dict)
            return self.update(out, **update_kwargs)

        # Otherwise, run both functions in separation.
        elif isinstance(edge_index, Tensor) or not self.fuse:
            coll_dict = self.__collect__(self.__user_args__, edge_index, size,
                                         kwargs)

            msg_kwargs = self.inspector.distribute('message', coll_dict)
            #print("Message kwargs: ",msg_kwargs)
            out = self.message(**msg_kwargs)

            # For `GNNExplainer`, we require a separate message and aggregate
            # procedure since this allows us to inject the `edge_mask` into the
            # message passing computation scheme.
            if self.__explain__:
                edge_mask = self.__edge_mask__.sigmoid()
                # Some ops add self-loops to `edge_index`. We need to do the
                # same for `edge_mask` (but do not train those).
                if out.size(self.node_dim) != edge_mask.size(0):
                    loop = edge_mask.new_ones(size[0])
                    edge_mask = torch.cat([edge_mask, loop], dim=0)
                assert out.size(self.node_dim) == edge_mask.size(0)
                out = out * edge_mask.view([-1] + [1] * (out.dim() - 1))

            aggr_kwargs = self.inspector.distribute('aggregate', coll_dict)
            out = self.aggregate(out, **aggr_kwargs)

            update_kwargs = self.inspector.distribute('update', coll_dict)
            return self.update(out, **update_kwargs)
        
        
    def forward(self, x, edge_index):
        # x has shape [N, in_channels]
        # edge_index has shape [2, E]

        # Step 1: Add self-loops to the adjacency matrix.
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))

        # Step 2: Linearly transform node feature matrix.
        x = self.lin(x)

        # Compute degree.
        row, col = edge_index
        deg = degree(col, x.size(0), dtype=x.dtype)
        adjmat = SparseTensor(row=edge_index[0], col=edge_index[1], value=torch.ones(edge_index.shape[1]))
        
        return self.propagate(adjmat, x=x, deg=deg.view((-1, 1)))
        


    def message(self, x_j, deg_i,deg_j):
        # Accoding to __collect__ function 
        # in https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/nn/conv/message_passing.py
        # when flow = source_to_target
        # i= 1, j=0, edge_index_i = edge_index[1] = target, so 
        # deg_i is degree of target node,  and x_i is target node data
        # deg_j is degree of source node and  x_j is source 
        # x_j has shape [E, out_channels]
        # deg_i has shape [E, 1]
        
        
        # Step 3: Normalize node features.
        print("--message is called--")
        print("x_j: ",x_j.shape)
        print("degree: ", deg_i.shape)
        print("degree: ",deg_j.shape)
        print()
        # check if degrees of source nodes and degrees of target nodes are equal
        print(torch.eq(deg_i, deg_j).all())
        # compute normalization
        deg_i = deg_i.pow(-0.5)
        deg_j = deg_j.pow(-0.5)
        norm = deg_i * deg_j
        
        return norm.view(-1, 1) * x_j
    
    def aggregate(self, inputs, index, ptr, dim_size):
        #from __collect__() function we know that
        # when flow = source_to_target
        # out['index'] = out['edge_index_i']  -> input index = edge_index[i] = edge_index[1] = index of target node
        # inputs: embedding vectors of source nodes
        # inputs: the outputs from message function, the normalized source node embeding with shape [E, dim of embedding]
        
        print("--aggregate` is called--")
        print('self.aggr:', self.aggr)
        print('ptr: ', ptr)
        print('dim_size: ',dim_size)
        print("inputs: ", inputs.shape)
        print("index: ",index.shape, len(index.unique()))
        print()
        uni_idx = index.unique()
        uni_idx.sort()
        
        res= []
        # find all unique target node index
        # for each target node, aggregate(sum or mean ) the information from source node to the target node
        # and obtain target node embedding
        for i in uni_idx:
            # i is the index of target node
            neighbors = inputs[index == i]
            # aggregate along different vectors of different nodes
            if self.aggr=="mean":
                agg_res = neighbors.mean(axis=0)
            else:
                agg_res = neighbors.sum(axis=0)
            res.append(agg_res)
        res = torch.stack(res)
        return res 
    
    
    def message_and_aggregate(self, adj_t, x_j, index,deg_i, deg_j):
        # note: 
        # adj_t: adjacency matrix
        # norm: normalization coefficient 1/sqrt(deg_i)*sqrt(deg_j)
        # number of '1' in adj_t = length of norm
        
        ## Print something to debug
        #print('`message_and_aggregate` is called')
        #print("adj_t: ",adj_t)
        #print("deg:", deg)
        print("--message_and_aggregate is called --")

        # Step3:  compute normalization
        deg_i = deg_i.pow(-0.5)
        deg_j = deg_j.pow(-0.5)
        norm = deg_i * deg_j
        
        # Step4: compute normalized message
        inputs = norm.view(-1, 1) * x_j
        
        # Step5: aggregate function sum
        uni_idx = index.unique()
        uni_idx.sort()
        
        res= []
        # find all unique target node index
        # for each target node, aggregate(sum or mean ) the information from source node to the target node
        # and obtain target node embedding
        for i in uni_idx:
            # i is the index of target node
            neighbors = inputs[index == i]
            # aggregate along different vectors of different nodes
            if self.aggr=="mean":
                agg_res = neighbors.mean(axis=0)
            else:
                agg_res = neighbors.sum(axis=0)
            res.append(agg_res)
        res = torch.stack(res)
        
        return res
    
    def update(self,inputs, deg ):
        print("--update func is called--")
        return self.lin2(inputs)

dataset = Planetoid(root='dataset/Cora', name='Cora')
data = dataset[0]

net = GCNConv(data.num_features, 64)
h_nodes = net(data.x, data.edge_index)
print("H_nodes: ", h_nodes.shape)
h_nodes
```

    --message_and_aggregate is called --
    --update func is called--
    H_nodes:  torch.Size([2708, 64])





    tensor([[-0.0301, -0.0607, -0.0843,  ..., -0.0092,  0.0735,  0.1196],
            [-0.0287, -0.0805, -0.0924,  ..., -0.0665,  0.0596,  0.0680],
            [-0.0236, -0.0952, -0.1220,  ..., -0.0735,  0.0296,  0.0909],
            ...,
            [-0.0257, -0.0769, -0.0840,  ..., -0.0068,  0.0807,  0.1330],
            [-0.0402, -0.0765, -0.1098,  ..., -0.0396,  0.0407,  0.1058],
            [-0.0421, -0.0787, -0.1024,  ..., -0.0455,  0.0361,  0.1054]],
           grad_fn=<AddmmBackward>)




```python

```

#### 5.3 **è®¾è®¡è‡ªå®šä¹‰ä¸€ä¸ªGCN layer**
è¿™é‡Œæˆ‘è‡ªå®šä¹‰çš„GCN layerå…¬å¼å¦‚ä¸‹ï¼š
$$
\mathbf{x}_i^{(k)} = \sigma(\frac{1}{|\mathcal{N}(i)|+1} \times \sum_{j \in \mathcal{N}(i) \cup \{ i \}} \frac{1}{\sqrt{\deg(i)} \cdot \sqrt{\deg(j)}} \cdot \left( \mathbf{\Theta} \cdot \mathbf{x}_j^{(k-1)} \right) ) +  \mathbf{\Theta}  \cdot \mathbf{x}_i^{(k-1)} ,
$$

è¿™é‡Œä¸€äº›å‡½æ•°å®šä¹‰å¦‚ä¸‹ï¼š
+ $\phi(..)$: messageå‡½æ•°å’Œä¹‹å‰çš„GCNä¸€æ ·éƒ½æ˜¯linear projectionä¹‹åç”¨degreeè¿›è¡Œnormalization
+ $\square(..)$ : aggregate å‡½æ•° ç”¨æ¥mean
+ $\gamma(..)$: update å‡½æ•°æ˜¯å…ˆç”¨äº†ReLu activationå‡½æ•°, åœ¨åŠ ä¸ŠshortcutæŠŠä¹‹å‰æŠ•æ˜ ä¹‹åçš„è¾“å…¥åŠ ä¸Šæ¥ï¼Œæ¨¡æ‹Ÿäº†resnetçš„ç»“æ„
+ è¿™é‡Œåªç”¨äº† message_and_aggregate å‡½æ•°ï¼Œæ‰€ä»¥æ²¡æœ‰å®ç°messageï¼Œ aggregateçš„å•ç‹¬çš„å‡½æ•°
+ propagate å‡½æ•°æ˜¯ç›´æ¥ä»å®˜æ–¹æ–‡æ¡£copyè¿‡æ¥ï¼Œæ–¹ä¾¿ç†è§£GNNçš„propagateçš„æµç¨‹çš„ã€‚ ä»ä¸­å¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœè¾“å…¥åˆ°propagateçš„tensoræ˜¯SparseTensor, é‚£ä¹ˆä¼šç›´æ¥è°ƒç”¨message_and_aggregateå‡½æ•°ï¼Œè€Œä¸æ˜¯å•ç‹¬è°ƒç”¨ä¸¤ä¸ªå‡½æ•°ï¼Œæ‰€ä»¥åªè¦å®ç°è¿™ä¸ªåˆå¹¶çš„å‡½æ•°å°±è¡Œäº†


```python
from torch_geometric.datasets import Planetoid
import torch
from torch import nn, Tensor
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops, degree
from torch_sparse import SparseTensor, matmul


class MyGCNConv(MessagePassing):
    def __init__(self, in_channels, out_channels):
        super(MyGCNConv, self).__init__(aggr='mean', flow='source_to_target')
        # "Add" aggregation (Step 5).
        # flow='source_to_target' è¡¨ç¤ºæ¶ˆæ¯ä»æºèŠ‚ç‚¹ä¼ æ’­åˆ°ç›®æ ‡èŠ‚ç‚¹
        self.lin = torch.nn.Linear(in_channels, out_channels)
        self.relu = torch.nn.ReLU()

        
    def propagate(self, edge_index, size=None, **kwargs):
        # I just copy the source copy from PyG website
        r"""The initial call to start propagating messages.

        Args:
            edge_index (Tensor or SparseTensor): A :obj:`torch.LongTensor` or a
                :obj:`torch_sparse.SparseTensor` that defines the underlying
                graph connectivity/message passing flow.
                :obj:`edge_index` holds the indices of a general (sparse)
                assignment matrix of shape :obj:`[N, M]`.
                If :obj:`edge_index` is of type :obj:`torch.LongTensor`, its
                shape must be defined as :obj:`[2, num_messages]`, where
                messages from nodes in :obj:`edge_index[0]` are sent to
                nodes in :obj:`edge_index[1]`
                (in case :obj:`flow="source_to_target"`).
                If :obj:`edge_index` is of type
                :obj:`torch_sparse.SparseTensor`, its sparse indices
                :obj:`(row, col)` should relate to :obj:`row = edge_index[1]`
                and :obj:`col = edge_index[0]`.
                The major difference between both formats is that we need to
                input the *transposed* sparse adjacency matrix into
                :func:`propagate`.
            size (tuple, optional): The size :obj:`(N, M)` of the assignment
                matrix in case :obj:`edge_index` is a :obj:`LongTensor`.
                If set to :obj:`None`, the size will be automatically inferred
                and assumed to be quadratic.
                This argument is ignored in case :obj:`edge_index` is a
                :obj:`torch_sparse.SparseTensor`. (default: :obj:`None`)
            **kwargs: Any additional data which is needed to construct and
                aggregate messages, and to update node embeddings.
        """
        size = self.__check_input__(edge_index, size)

        # Run "fused" message and aggregation (if applicable).
        if (isinstance(edge_index, SparseTensor) and self.fuse
                and not self.__explain__):
            coll_dict = self.__collect__(self.__fused_user_args__, edge_index,
                                         size, kwargs)
            print("Using self-defined message-passing")
            msg_aggr_kwargs = self.inspector.distribute(
                'message_and_aggregate', coll_dict)
            out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs)

            update_kwargs = self.inspector.distribute('update', coll_dict)
            return self.update(out, **update_kwargs)

        # Otherwise, run both functions in separation.
        elif isinstance(edge_index, Tensor) or not self.fuse:
            coll_dict = self.__collect__(self.__user_args__, edge_index, size,
                                         kwargs)

            msg_kwargs = self.inspector.distribute('message', coll_dict)
            out = self.message(**msg_kwargs)

            # For `GNNExplainer`, we require a separate message and aggregate
            # procedure since this allows us to inject the `edge_mask` into the
            # message passing computation scheme.
            if self.__explain__:
                edge_mask = self.__edge_mask__.sigmoid()
                # Some ops add self-loops to `edge_index`. We need to do the
                # same for `edge_mask` (but do not train those).
                if out.size(self.node_dim) != edge_mask.size(0):
                    loop = edge_mask.new_ones(size[0])
                    edge_mask = torch.cat([edge_mask, loop], dim=0)
                assert out.size(self.node_dim) == edge_mask.size(0)
                out = out * edge_mask.view([-1] + [1] * (out.dim() - 1))

            aggr_kwargs = self.inspector.distribute('aggregate', coll_dict)
            out = self.aggregate(out, **aggr_kwargs)

            update_kwargs = self.inspector.distribute('update', coll_dict)
            return self.update(out, **update_kwargs)
        
        
    def forward(self, x, edge_index):
        # x has shape [N, in_channels]
        # edge_index has shape [2, E]

        # Step 1: Add self-loops to the adjacency matrix.
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))

        # Step 2: Linearly transform node feature matrix.
        x = self.lin(x)

        # Step 3: Compute normalization.
        row, col = edge_index
        deg = degree(col, x.size(0), dtype=x.dtype)
        deg_inv_sqrt = deg.pow(-0.5)
        # note: norm is in shape of (number of edge, )
        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]
        print("Get degree Shape: ", edge_index.shape)
        print("Norm Shape: ",norm.shape)
        
        # Step 4-5: Start propagating messages.
        # Convert edge index to a sparse adjacency matrix representation, with row = from nodes, col = to nodes. 
        # When value =  1 in adjacency matrix, it indicates two nodes are adjacent.
        # adjmat = SparseTensor(row=edge_index[0], col=edge_index[1], value=torch.ones(edge_index.shape[1]))
        
        # è¿™é‡Œ adjacency matrix çš„å€¼ä»1 å˜æˆ normalization çš„å€¼ï¼Œæ–¹ä¾¿ä¹˜æ³•è®¡ç®—
        adjmat = SparseTensor(row=edge_index[0], col=edge_index[1], value=norm)
        
        # æ­¤å¤„ä¼ çš„ä¸å†æ˜¯edge_idexï¼Œè€Œæ˜¯SparseTensorç±»å‹çš„Adjancency Matrix
        return self.propagate(adjmat, x=x, norm=norm, deg=deg.view((-1, 1)))


    def message(self, x_j, norm, deg_i):
        # x_j has shape [E, out_channels]
        # deg_i has shape [E, 1]
        # Step 4: Normalize node features.
        
        return norm.view(-1, 1) * x_j * deg_i

    def aggregate(self, inputs, index, ptr, dim_size):
        print('self.aggr:', self.aggr)
        print("`aggregate` is called")
        return super().aggregate(inputs, index, ptr=ptr, dim_size=dim_size)

    def message_and_aggregate(self, adj_t, x, norm,deg):
        # note: 
        # adj_t: adjacency matrix
        # norm: normalization coefficient 1/sqrt(deg_i)*sqrt(deg_j)
        # number of '1' in adj_t = length of norm
        
        ## Print something to debug
        #print('`message_and_aggregate` is called')
        #print("adj_t: ",adj_t)
        #print("deg:", deg)
        
        adj_t = adj_t.to_dense()
        N = len(adj_t)
        out = []
        x0 = x[:]
        for i in range(N):
            # è®¡ç®—æ¯ä¸ª xi çš„neighborä¼ è¿‡æ¥çš„ä¿¡æ¯çš„å¹³å‡å€¼
            x_sum = torch.matmul(x.T,adj_t[i])
            x_avg = x_sum/deg[i]
            out.append(x_avg)
        out = torch.stack(out)
        return [out, x0]

    def update(self, inputs, deg):
        print("Update result")
        print("Degree",deg)
        # resnetçš„ç»“æ„
        x0 = inputs[1]
        output = self.relu(inputs[0]) + x0
        return output


dataset = Planetoid(root='dataset/Cora', name='Cora')
data = dataset[0]

net = MyGCNConv(data.num_features, 64)
h_nodes = net(data.x, data.edge_index)

```

    Get degree Shape:  torch.Size([2, 13264])
    Norm Shape:  torch.Size([13264])
    Using self-defined message-passing
    Update result
    Degree tensor([[4.],
            [4.],
            [6.],
            ...,
            [2.],
            [5.],
            [5.]])



```python
h_nodes
```




    tensor([[-2.4017e-02,  4.7570e-02,  1.1954e-02,  ...,  1.3043e-02,
              2.0967e-02, -8.4416e-02],
            [-8.5681e-02,  1.2029e-01,  1.0756e-01,  ...,  5.4046e-02,
             -8.9611e-02, -1.9092e-01],
            [ 6.2691e-02, -2.7604e-02, -6.0106e-02,  ..., -3.0790e-05,
              7.8295e-03, -7.2708e-02],
            ...,
            [ 2.0562e-02,  6.4994e-02,  1.0240e-01,  ..., -3.2108e-03,
              6.4759e-02,  1.3680e-02],
            [-1.9234e-02, -2.0179e-02,  3.0165e-02,  ..., -1.4412e-01,
             -4.2793e-02, -5.4195e-02],
            [-2.6318e-02, -2.6606e-02,  9.8404e-02,  ..., -5.1031e-02,
             -2.9973e-02,  1.8722e-02]], grad_fn=<AddBackward0>)




```python

```
