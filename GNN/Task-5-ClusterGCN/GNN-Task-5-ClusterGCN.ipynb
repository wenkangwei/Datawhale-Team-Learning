{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN-Task-5-Cluster GCN\n",
    "\n",
    "## 1. Introduction\n",
    "这篇文章的目的主要是理解 Cluster-GCN这篇文章的内容(要解决的问题，思路，等)和通过代码实现一下Cluster-GCN网络。 Cluster-GCN的文章可以查看: https://arxiv.org/pdf/1905.07953.pdf\n",
    "这篇blog的结构大概如下:\n",
    "+ 解释Cluster-GCN的内容\n",
    "    - Cluster-GCN要解决的问题\n",
    "    - 基本思路\n",
    "    - Cluster-GCN的特点和优缺点\n",
    "+ Coding实现\n",
    "    - 数据集\n",
    "    - Cluster-GCN模型\n",
    "    - Training and Testing\n",
    "    - Assignment from datawhale\n",
    "+ 总结文章的重点\n",
    "+ Reference 参考文献\n",
    "\n",
    "## 2. Cluster-GCN\n",
    "Cluster GCN是由国立台湾大学Wei-Lin Chiang，Google research 的Yang Li和 Samy Bengio， Cho-Jui Hsieh 等人提出的GCN网络(看到Bengio等Google大牛的名字就知道这篇文章很值得一读)。文章的全称是Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks。 从名字就可以知道这个Cluster-GCN的目的是要优化深度学习和超大图网络的效率而提出来的(一般都会涉及时间和空间的复杂度分析)。这里先来看看这篇文章要解决的问题\n",
    "\n",
    "### 2.1 Problem to solve\n",
    "首先这篇文章提出和分析了在GNN学习里面的几个问题:\n",
    "+ Full-batch gradient descent:\n",
    "\n",
    "    这里先做以下定义\n",
    "    - node的个数为N\n",
    "    - Embedding dimension = F\n",
    "    - GCN的layer层数为L.\n",
    "    \n",
    "    那么在用Full batch GD 全梯度下降方法时所需的Space Complexity = O(NFL)并且计算梯度时如果node个数很多有上百万个，每个epoch里面梯度计算也是很慢的。因此这种方法不考虑\n",
    "    \n",
    "+ **Mini-batch SGD**\n",
    "    \n",
    "    Mini-batch SGD算是对Full batch GD的方法通过随机采样降低要计算的梯度的存储空间以及加快了计算的速度。 但是mini-batch SGD会使得时间复杂度随着GCN的层数增加而指数级增长。paper原话是这么解释的:\n",
    "    `\n",
    "    mini-batch SGD introduces a significant computational overhead due to the neighborhood expansion problem—to compute the loss on a single node at layer L, it requires that node’s neighbor nodes’ embeddings at layer L − 1, which again requires their neighbors’ embeddings at layer L − 2 and recursive ones in the downstream layers. This leads to time complexity exponential to the GCN depth.\n",
    "    `\n",
    "    用我自己的话来理解就是，以下图为例\n",
    "    <img src=./graph-1.png>\n",
    "\n",
    "         如果在GCN第i层的第j个节点的embedding vector用$Z_ {i}^{j}$，而第i个节点的neighbor用$N(i)$表示， 那么在第0层输入层节点A的embedding就是$Z_ {A}^{0}$, $N(A)$ = {B,D}。那么我们就有以下的公式:\n",
    "\n",
    "    - 在第1层 L1, $Z_ {A}^{1} = f(Z_ {B}^{0}, Z_ {D}^{0}, Z_ {A}^{0})$\n",
    "    - 在第2层 L2, $Z_ {A}^{2} = f(Z_ {B}^{1}, Z_ {D}^{1}, Z_ {A}^{1})$, 而其中又可以把$Z_ {A}^{1}$ 展开成L1层里面的式子，$Z_ {B}^{1},Z_ {D}^{1}$ 同理\n",
    "    - 第3层L3,  $Z_ {A}^{3} = f(Z_ {B}^{2}, Z_ {D}^{2}, Z_ {A}^{2})$, 计算时和第二步同理可以多次展开成用L1层的输入表示的形式，这么一来可以看到，随着GCN层数增加，neighborhood expansion problem 邻居展开问题就会使得梯度计算更加复杂\n",
    "    - 这个节点i在第j层梯度计算都取决于第j-1层前面一层的计算就是neighborhood expansion problem\n",
    "    - 如果GCN层数为L， 节点的平均degree为d，那么我们计算一个节点的梯度就需要$O(d ^L)$个节点embedding信息， 而由于要乘上权重矩阵W, 计算每个node的embedding需要O($F^2$) 的时间。 那么计算一个node的梯度为$O(d ^LF^2)$\n",
    "\n",
    "\n",
    "\n",
    "+ VR-GCN(variance reduction GCN), it uses variance reduction technique to reduce the size of neighborhood sampling nodes\n",
    "\n",
    "### 2.2 How Cluster-GCN works\n",
    "\n",
    "#### 2.2.1 Cluster-GCN的思想\n",
    "\n",
    "ClusterGCN的想法是我们能不能找到一把种将节点分成多个batch的方式，并将图划分成多个子图，使得表征利用率最大？我们通过将表征利用率的概念与图节点聚类的目标联系起来来回答这个问题。原文:`can we design a batch and the corresponding computation subgraph to maximize the embedding utilization?`\n",
    "\n",
    "\n",
    "**节点表征的利用率可以反映出计算的效率。**考虑到一个batch有多个节点，时间与空间复杂度的计算就不是上面那样简单了，因为不同的节点同样距离远的邻接节点可以是重叠的，于是计算表征的次数可以小于最坏的情况$O\\left(b d^{L}\\right)$。为了反映mini-batch SGD的计算效率，Cluster-GCN论文提出了**\"表征利用率\"**的概念来描述计算效率。在训练过程中，如果节点$i$在$l$层的表征$z_{i}^{(l)}$被计算并在$l+1$层的表征计算中被重复使用$u$次，那么我们说$z_{i}^{(l)}$的表征利用率为$u$。**对于随机抽样的mini-batch SGD，$u$非常小**，因为图通常是大且稀疏的。假设$u$是一个小常数（节点间同样距离的邻接节点重叠率小），那么mini-batch SGD的训练方式对每个batch需要计算$O\\left(b d^{L}\\right)$的表征，于是每次参数更新需要$O\\left(b d^{L} F^{2}\\right)$的时间，**每个epoch需要$O\\left(N d^{L} F^{2}\\right)$的时间**，这被称为**邻域扩展问题**。\n",
    "\n",
    "相反的是，**全梯度下降训练具有最大的表征利用率**——每个节点表征将在上一层被重复使用平均节点度次。因此，全梯度下降法在每个epoch中只需要计算$O(N L)$的表征，这意味着平均下来只需要$O(L)$的表征计算就可以获得一个节点的梯度。\n",
    "\n",
    "\n",
    "#### 2.2.2 简单的Cluster-GCN方法\n",
    "\n",
    "考虑到在每个batch中，我们计算一组节点（记为$\\mathcal{B}$）从第$1$层到第$L$层的表征。由于图神经网络每一层的计算都使用相同的子图$A_{\\mathcal{B}, \\mathcal{B}}$（$\\mathcal{B}$内部的边），所以表征利用率就是这个batch内边的数量，记为$\\left\\|A_{\\mathcal{B}, \\mathcal{B}}\\right\\|_{0}$。因此，**为了最大限度地提高表征利用率，理想的划分batch的结果是，batch内的边尽可能多，batch之间的边尽可能少**。基于这一点，我们将SGD图神经网络训练的效率与图聚类算法联系起来。\n",
    "\n",
    "**现在我们正式学习Cluster-GCN方法**。对于一个图$G$，我们将其节点划分为$c$个簇：$\\mathcal{V}=\\left[\\mathcal{V}_{1}, \\cdots \\mathcal{V}_{c}\\right]$，其中$\\mathcal{V}_{t}$由第$t$个簇中的节点组成，对应的我们有$c$个子图：\n",
    "$$\n",
    "\\bar{G}=\\left[G_{1}, \\cdots, G_{c}\\right]=\\left[\\left\\{\\mathcal{V}_{1}, \\mathcal{E}_{1}\\right\\}, \\cdots,\\left\\{\\mathcal{V}_{c}, \\mathcal{E}_{c}\\right\\}\\right]\n",
    "\\notag\n",
    "$$\n",
    "其中$\\mathcal{E}_{t}$只由$\\mathcal{V}_{t}$中的节点之间的边组成。经过节点重组，邻接矩阵被划分为大小为$c^{2}$的块矩阵，如下所示\n",
    "$$\n",
    "A=\\bar{A}+\\Delta=\\left[\\begin{array}{ccc}\n",
    "A_{11} & \\cdots & A_{1 c} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "A_{c 1} & \\cdots & A_{c c}\n",
    "\\end{array}\\right]\n",
    "\\tag{4}\n",
    "$$\n",
    "其中\n",
    "$$\n",
    "\\bar{A}=\\left[\\begin{array}{ccc}\n",
    "A_{11} & \\cdots & 0 \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "0 & \\cdots & A_{c c}\n",
    "\\end{array}\\right], \\Delta=\\left[\\begin{array}{ccc}\n",
    "0 & \\cdots & A_{1 c} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "A_{c 1} & \\cdots & 0\n",
    "\\end{array}\\right]\n",
    "\\tag{5}\n",
    "$$\n",
    "其中，对角线上的块$A_{t t}$是大小为$\\left|\\mathcal{V}_{t}\\right| \\times\\left|\\mathcal{V}_{t}\\right|$的邻接矩阵，它由$G_{t}$内部的边构成。$\\bar{A}$是图$\\bar{G}$的邻接矩阵。$A_{s t}$由两个簇$\\mathcal{V}_{s}$和$\\mathcal{V}_{t}$之间的边构成。$\\Delta$是由$A$的所有非对角线块组成的矩阵。同样，我们可以根据$\\left[\\mathcal{V}_{1}, \\cdots, \\mathcal{V}_{c}\\right]$划分节点表征矩阵$X$和类别向量$Y$，得到$\\left[X_{1}, \\cdots, X_{c}\\right]$和$\\left[Y_{1}, \\cdots, Y_{c}\\right]$，其中$X_{t}$和$Y_{t}$分别由$V_{t}$中节点的表征和类别组成。\n",
    "\n",
    "接下来我们**用块对角线邻接矩阵$\\bar{A}$去近似邻接矩阵$A$**，这样做的好处是，**完整的损失函数（公示(2）)可以根据batch分解成多个部分之和**。以$\\bar{A}^{\\prime}$表示归一化后的$\\bar{A}$，最后一层节点表征矩阵可以做如下的分解：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z^{(L)} &=\\bar{A}^{\\prime} \\sigma\\left(\\bar{A}^{\\prime} \\sigma\\left(\\cdots \\sigma\\left(\\bar{A}^{\\prime} X W^{(0)}\\right) W^{(1)}\\right) \\cdots\\right) W^{(L-1)} \\\\\n",
    "&=\\left[\\begin{array}{c}\n",
    "\\bar{A}_{11}^{\\prime} \\sigma\\left(\\bar{A}_{11}^{\\prime} \\sigma\\left(\\cdots \\sigma\\left(\\bar{A}_{11}^{\\prime} X_{1} W^{(0)}\\right) W^{(1)}\\right) \\cdots\\right) W^{(L-1)} \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{A}_{c c}^{\\prime} \\sigma\\left(\\bar{A}_{c c}^{\\prime} \\sigma\\left(\\cdots \\sigma\\left(\\bar{A}_{c c}^{\\prime} X_{c} W^{(0)}\\right) W^{(1)}\\right) \\cdots\\right) W^{(L-1)}\n",
    "\\end{array}\\right]\n",
    "\\end{aligned}\n",
    "\\tag{6}\n",
    "$$\n",
    "由于$\\bar{A}$是块对角形式（$\\bar{A}_{t t}^{\\prime}$是$\\bar{A}^{\\prime}$的对角线上的块），于是损失函数可以分解为\n",
    "$$\n",
    "\\mathcal{L}_{\\bar{A}^{\\prime}}=\\sum_{t} \\frac{\\left|\\mathcal{V}_{t}\\right|}{N} \\mathcal{L}_{\\bar{A}_{t t}^{\\prime}} \\text { and } \\mathcal{L}_{\\bar{A}_{t t}^{\\prime}}=\\frac{1}{\\left|\\mathcal{V}_{t}\\right|} \\sum_{i \\in \\mathcal{V}_{t}} \\operatorname{loss}\\left(y_{i}, z_{i}^{(L)}\\right)\n",
    "\\tag{7}\n",
    "$$\n",
    "基于公式(6)和公式(7)，在训练的每一步中，Cluster-GCN首先**采样一个簇$\\mathcal{V}_{t}$**，然后**根据$\\mathcal{L}_{{\\bar{A}^{\\prime}}_{tt}}$的梯度进行参数更新**。这种训练方式，只需要用到子图$A_{t t}$, $X_{t}$, $Y_{t}$以及神经网络权重矩阵$\\left\\{W^{(l)}\\right\\}_{l=1}^{L}$。 实际中，主要的计算开销在神经网络前向过程中的矩阵乘法运算（公式(6)的一个行）和梯度反向传播。\n",
    "\n",
    "我们使用图节点聚类算法来划分图。**图节点聚类算法将图节点分成多个簇，划分结果是簇内边的数量远多于簇间边的数量**。如前所述，每个batch的表征利用率相当于簇内边的数量。直观地说，每个节点和它的邻接节点大部分情况下都位于同一个簇中，因此**$L$跳（L-hop）远的邻接节点大概率仍然在同一个簇中**。由于我们用块对角线近似邻接矩阵$\\bar{A}$代替邻接矩阵$A$，产生的误差与簇间的边的数量$\\Delta$成正比，所以**簇间的边越少越好**。综上所述，使用图节点聚类算法对图节点划分多个簇的结果，正是我们希望得到的。\n",
    "\n",
    "在下图，我们可以看到，**Cluster-GCN方法可以避免巨大范围的邻域扩展**（图右），因为Cluster-GCN方法将邻域扩展限制在簇内。\n",
    "\n",
    "<img src=neighborhood-expansion.png>\n",
    "\n",
    "#### 2.2.3 Cluster-GCN实现过程\n",
    "<img src=alg-1.png>\n",
    "\n",
    "从上图可以看到, Cluster-GCN的实现流程基本是\n",
    "1. 用METIS partition算法对图的节点进行分解成c个cluster\n",
    "2. 不断迭代， 每次迭代都随机选取q个cluster进行无放回采样node，links并形成subgraph\n",
    "3. 对subgraph进行预测和gradient计算\n",
    "4. 用adam进行node的更新和学习\n",
    "\n",
    "另外 Cluster-GCN方法提出了一个修改版的公式(9)，以更好地保持邻接节点信息和数值范围。首先给原始的$A$添加一个单位矩阵$I$，并进行归一化处理\n",
    "$$\n",
    "\\tilde{A}=(D+I)^{-1}(A+I)\n",
    "\\tag{10}\n",
    "$$\n",
    "然后考虑，\n",
    "$$\n",
    "X^{(l+1)}=\\sigma\\left((\\tilde{A}+\\lambda \\operatorname{diag}(\\tilde{A})) X^{(l)} W^{(l)}\\right)\n",
    "\\tag{11}\n",
    "$$\n",
    "\n",
    "以上就是Cluster-GCN的每层layer的更新输出公式\n",
    "\n",
    "### 2.3 Properties\n",
    "+ Advantage\n",
    "    - 先来看看时间和空间复杂度。 在时间上它只和layer层数 L, embedding feature的大小F以及邻接矩阵的非零的行数||A||和节点个数有关， 而空间上和batch的大小相关，相对于传统的GCN，它把指数次降到1次\n",
    "    \n",
    "    <img src=Complexity.png>\n",
    "    \n",
    "    - 除了Time, Space complexity外, paper里面提及在大型的图数据里面如PPI, Reddit是最好的(这个可能有调参的因素在里面)\n",
    "    \n",
    "    <img src=SOTA-result.png>\n",
    "    \n",
    "+ Shortage\n",
    "    - 在收敛性上面, Cluster-GCN在layer数量超过3层之后Accuracy性能没有明显变大，反而layer到了6层之后，开始收敛不好性能变差,如下图所示\n",
    "    <img src=convergence.png>\n",
    "    \n",
    "+ Other properties\n",
    "    - ClusterGCN在做clustering对节点进行cluster partition时特意对比了 random partition和METIS clustering partition两种方法, 以及batch设计时是用多个cluster还是一个cluster作为一个batch。它表明了用METIS和 multiple clusters as a batch 更能使性能提升，loss降低更多。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 官方测试源码\n",
    "这里用了Reddit的dataset进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://data.dgl.ai/dataset/reddit.zip\n",
      "Extracting data/Reddit/raw/reddit.zip\n",
      "Processing...\n",
      "Done!\n",
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.0684\n",
      "Epoch: 02, Loss: 0.4532\n",
      "Epoch: 03, Loss: 0.3874\n",
      "Epoch: 04, Loss: 0.3552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [00:42<00:00, 10886.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Loss: 0.3361, Train: 0.9568, Val: 0.9523, test: 0.9509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Loss: 0.3220\n",
      "Epoch: 07, Loss: 0.3259\n",
      "Epoch: 08, Loss: 0.3068\n",
      "Epoch: 09, Loss: 0.2899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [00:43<00:00, 10825.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.2844, Train: 0.9639, Val: 0.9517, test: 0.9523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss: 0.2850\n",
      "Epoch: 12, Loss: 0.2700\n",
      "Epoch: 13, Loss: 0.2705\n",
      "Epoch: 14, Loss: 0.2696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [00:43<00:00, 10803.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss: 0.2793, Train: 0.9637, Val: 0.9524, test: 0.9506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Loss: 0.2699\n",
      "Epoch: 17, Loss: 0.2556\n",
      "Epoch: 18, Loss: 0.2656\n",
      "Epoch: 19, Loss: 0.2642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [00:43<00:00, 10797.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Loss: 0.2491, Train: 0.9686, Val: 0.9551, test: 0.9537\n",
      "Epoch: 21, Loss: 0.2450\n",
      "Epoch: 22, Loss: 0.2449\n",
      "Epoch: 23, Loss: 0.2456\n",
      "Epoch: 24, Loss: 0.2491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [00:43<00:00, 10737.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Loss: 0.2518, Train: 0.9572, Val: 0.9433, test: 0.9389\n",
      "Epoch: 26, Loss: 0.2430\n",
      "Epoch: 27, Loss: 0.2342\n",
      "Epoch: 28, Loss: 0.2297\n",
      "Epoch: 29, Loss: 0.2270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [00:43<00:00, 10824.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Loss: 0.2319, Train: 0.9716, Val: 0.9514, test: 0.9517\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.datasets import Reddit\n",
    "from torch_geometric.data import ClusterData, ClusterLoader, NeighborSampler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "dataset = Reddit('./data/Reddit')\n",
    "data = dataset[0]\n",
    "\n",
    "cluster_data = ClusterData(data, num_parts=1500, recursive=False,\n",
    "                           save_dir=dataset.processed_dir)\n",
    "train_loader = ClusterLoader(cluster_data, batch_size=20, shuffle=True,\n",
    "                             num_workers=12)\n",
    "\n",
    "subgraph_loader = NeighborSampler(data.edge_index, sizes=[-1], batch_size=1024,\n",
    "                                  shuffle=False, num_workers=12)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Net, self).__init__()\n",
    "        self.convs = ModuleList(\n",
    "            [SAGEConv(in_channels, 128),\n",
    "             SAGEConv(128, out_channels)])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != len(self.convs) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def inference(self, x_all):\n",
    "        pbar = tqdm(total=x_all.size(0) * len(self.convs))\n",
    "        pbar.set_description('Evaluating')\n",
    "\n",
    "        # Compute representations of nodes layer by layer, using *all*\n",
    "        # available edges. This leads to faster computation in contrast to\n",
    "        # immediately computing the final representations of each batch.\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            xs = []\n",
    "            for batch_size, n_id, adj in subgraph_loader:\n",
    "                edge_index, _, size = adj.to(device)\n",
    "                x = x_all[n_id].to(device)\n",
    "                x_target = x[:size[1]]\n",
    "                x = conv((x, x_target), edge_index)\n",
    "                if i != len(self.convs) - 1:\n",
    "                    x = F.relu(x)\n",
    "                xs.append(x.cpu())\n",
    "\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        return x_all\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(dataset.num_features, dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_nodes = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = F.nll_loss(out[batch.train_mask], batch.y[batch.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        nodes = batch.train_mask.sum().item()\n",
    "        total_loss += loss.item() * nodes\n",
    "        total_nodes += nodes\n",
    "\n",
    "    return total_loss / total_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():  # Inference should be performed on the full graph.\n",
    "    model.eval()\n",
    "\n",
    "    out = model.inference(data.x)\n",
    "    y_pred = out.argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = y_pred[mask].eq(data.y[mask]).sum().item()\n",
    "        accs.append(correct / mask.sum().item())\n",
    "    return accs\n",
    "\n",
    "\n",
    "for epoch in range(1, 31):\n",
    "    loss = train()\n",
    "    if epoch % 5 == 0:\n",
    "        train_acc, val_acc, test_acc = test()\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, '\n",
    "              f'Val: {val_acc:.4f}, test: {test_acc:.4f}')\n",
    "    else:\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 自己调整的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "41\n",
      "232965\n",
      "114615892\n",
      "602\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WITH_METIS\"] =\"1\"\n",
    "print(os.getenv('WITH_METIS'))\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.nn import ModuleList, functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.datasets import Reddit\n",
    "from torch_geometric.data import ClusterData, ClusterLoader, NeighborSampler\n",
    "\n",
    "dataset = Reddit('./data/Reddit')\n",
    "data = dataset[0]\n",
    "print(dataset.num_classes)\n",
    "print(data.num_nodes)\n",
    "print(data.num_edges)\n",
    "print(data.num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterGCNNet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,hidden_dim= 128,num_layers=4):\n",
    "        super(ClusterGCNNet, self).__init__()\n",
    "        # GraphSAGE layer\n",
    "        # 这里参考了 原paper里面的4-layer的设定 + 128 hidden units\n",
    "        layer_ls=[SAGEConv(in_channels, hidden_dim)]\n",
    "        if num_layers <=2:\n",
    "            layer_ls += [SAGEConv(hidden_dim, hidden_dim) for i in range(num_layers-2)]\n",
    "        layer_ls.append(SAGEConv(hidden_dim, out_channels))\n",
    "        self.convs = ModuleList(layer_ls)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != len(self.convs) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def inference(self, x_all, subgraph_loader,device):\n",
    "        pbar = tqdm(total=x_all.size(0) * len(self.convs))\n",
    "        pbar.set_description('Evaluating')\n",
    "\n",
    "        # Compute representations of nodes layer by layer, using *all*\n",
    "        # available edges. This leads to faster computation in contrast to\n",
    "        # immediately computing the final representations of each batch.\n",
    "        \n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            xs = []\n",
    "            for batch_size, n_id, adj in subgraph_loader:\n",
    "                edge_index, _, size = adj.to(device)\n",
    "                x = x_all[n_id].to(device)\n",
    "                x_target = x[:size[1]]\n",
    "                x = conv((x, x_target), edge_index)\n",
    "                if i != len(self.convs) - 1:\n",
    "                    x = F.relu(x)\n",
    "                xs.append(x.cpu())\n",
    "\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        return x_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model,optimizer):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_nodes = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = F.nll_loss(out[batch.train_mask], batch.y[batch.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        nodes = batch.train_mask.sum().item()\n",
    "        total_loss += loss.item() * nodes\n",
    "        total_nodes += nodes\n",
    "\n",
    "    return total_loss / total_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data, model):  # Inference should be performed on the full graph.\n",
    "    model.eval()\n",
    "\n",
    "    out = model.inference(data.x,subgraph_loader,device)\n",
    "    y_pred = out.argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = y_pred[mask].eq(data.y[mask]).sum().item()\n",
    "        accs.append(correct / mask.sum().item())\n",
    "    return accs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "# def test_num_parts(data, num_partitions = [500, 1000, 1500,2000],num_layers=2):\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "for cnt in range(1):\n",
    "    num_partitions = [1000, 1500,2000]\n",
    "    num_layers=2\n",
    "    torch.manual_seed(2019)\n",
    "    result = {\"num_cluster\":[],\"partition_t\":[],\"train_t\":[],\"train_acc\":[] ,\"val_acc\":[],\"test_acc\":[]}\n",
    "    \n",
    "    for num_part in num_partitions:\n",
    "        \n",
    "        \n",
    "        print(f\"Testing number of cluster:{num_part}\")\n",
    "        model = ClusterGCNNet(dataset.num_features, dataset.num_classes,num_layers=num_layers).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "        # 如果recursive = True， 它会自动用多层的二分法进行partition而不是多层k-way 分法，这样会比multilevel k-way 要慢\n",
    "        # 可以参考paper: http://glaros.dtc.umn.edu/gkhome/node/81\n",
    "        \n",
    "        start_t = time.time()\n",
    "        cluster_gnn_data = ClusterData(data, num_parts =num_part, recursive=False, save_dir = dataset.processed_dir )\n",
    "        end_t = time.time()\n",
    "        partition_t =end_t - start_t\n",
    "        print(f\"Partition Time: {partition_t} s\")\n",
    "\n",
    "        \n",
    "        train_loader = ClusterLoader(cluster_gnn_data, batch_size= 20, shuffle= True, num_workers= 16)\n",
    "\n",
    "        # 采样子图的edges， 这里subgraph_loader 用于 testing里面 取样子图来测试model的infernece\n",
    "        subgraph_loader = NeighborSampler(data.edge_index, sizes = [-1], shuffle=False,num_workers=16)\n",
    "\n",
    "        start_t = time.time()\n",
    "        for epoch in range(1, 31):\n",
    "            loss = train(model,optimizer)\n",
    "            if epoch % 5 == 0:\n",
    "                train_acc, val_acc, test_acc = test(data, model)\n",
    "                print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, '\n",
    "                      f'Val: {val_acc:.4f}, test: {test_acc:.4f}')\n",
    "            else:\n",
    "                print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "        end_t = time.time()\n",
    "        train_t =end_t - start_t\n",
    "        train_t = datetime.timedelta(seconds=train_t)\n",
    "        print(f\"Training Time: {train_t} s\")\n",
    "        \n",
    "        result[\"num_cluster\"].append(num_part)\n",
    "        result['partition_t'].append(partition_t)\n",
    "        result['train_t'].append(train_t)\n",
    "        result[\"train_acc\"].append(train_acc) \n",
    "        result[\"val_acc\"].append(val_acc)\n",
    "        result[\"test_acc\"].append(test_acc)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        del model\n",
    "        #return result, pd.DataFrame(result)\n",
    "df_res= pd.DataFrame(result)\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using num cluster: 500\n",
      "Partition Time: 1.7125461101531982 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.8346\n",
      "Epoch: 02, Loss: 0.6928\n",
      "Epoch: 03, Loss: 0.4838\n",
      "Epoch: 04, Loss: 0.4009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [00:57<00:00, 8127.96it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Loss: 0.3632, Train: 0.9541, Val: 0.9542, test: 0.9531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Loss: 0.3425\n",
      "Epoch: 07, Loss: 0.3161\n",
      "Epoch: 08, Loss: 0.3140\n",
      "Epoch: 09, Loss: 0.3006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [00:56<00:00, 8212.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.2787, Train: 0.9637, Val: 0.9586, test: 0.9571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss: 0.2684\n",
      "Epoch: 12, Loss: 0.2558\n",
      "Epoch: 13, Loss: 0.2522\n",
      "Epoch: 14, Loss: 0.2455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [01:00<00:00, 7730.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss: 0.2421, Train: 0.9667, Val: 0.9577, test: 0.9565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Loss: 0.2649\n",
      "Epoch: 17, Loss: 0.2377\n",
      "Epoch: 18, Loss: 0.2277\n",
      "Epoch: 19, Loss: 0.2190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [00:54<00:00, 8495.90it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Loss: 0.2151, Train: 0.9698, Val: 0.9568, test: 0.9559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Loss: 0.2138\n",
      "Epoch: 22, Loss: 0.2101\n",
      "Epoch: 23, Loss: 0.2084\n",
      "Epoch: 24, Loss: 0.2062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [00:58<00:00, 7914.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Loss: 0.2057, Train: 0.9713, Val: 0.9558, test: 0.9545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Loss: 0.2061\n",
      "Epoch: 27, Loss: 0.2097\n",
      "Epoch: 28, Loss: 0.2099\n",
      "Epoch: 29, Loss: 0.2035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [00:59<00:00, 7891.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Loss: 0.1938, Train: 0.9736, Val: 0.9567, test: 0.9560\n",
      "Training Time: 0:09:43.498993 s\n",
      "Using num cluster: 1000\n",
      "Partition Time: 3.144443988800049 s\n",
      "Epoch: 01, Loss: 1.4359\n",
      "Epoch: 02, Loss: 0.5340\n",
      "Epoch: 03, Loss: 0.4172\n",
      "Epoch: 04, Loss: 0.3630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [00:58<00:00, 7984.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Loss: 0.3458, Train: 0.9358, Val: 0.9350, test: 0.9327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Loss: 0.3326\n",
      "Epoch: 07, Loss: 0.3068\n",
      "Epoch: 08, Loss: 0.2879\n",
      "Epoch: 09, Loss: 0.2861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [00:59<00:00, 7885.83it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.2757, Train: 0.9618, Val: 0.9536, test: 0.9506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss: 0.2700\n",
      "Epoch: 12, Loss: 0.2535\n",
      "Epoch: 13, Loss: 0.2523\n",
      "Epoch: 14, Loss: 0.2461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [00:59<00:00, 7813.02it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss: 0.2412, Train: 0.9688, Val: 0.9568, test: 0.9548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Loss: 0.2470\n",
      "Epoch: 17, Loss: 0.2456\n",
      "Epoch: 18, Loss: 0.2403\n",
      "Epoch: 19, Loss: 0.2296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [00:54<00:00, 8482.81it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Loss: 0.2284, Train: 0.9696, Val: 0.9546, test: 0.9545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Loss: 0.2276\n",
      "Epoch: 22, Loss: 0.2219\n",
      "Epoch: 23, Loss: 0.2224\n",
      "Epoch: 24, Loss: 0.2233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [00:57<00:00, 8037.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Loss: 0.2241, Train: 0.9698, Val: 0.9533, test: 0.9520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Loss: 0.2226\n",
      "Epoch: 27, Loss: 0.2129\n",
      "Epoch: 28, Loss: 0.2171\n",
      "Epoch: 29, Loss: 0.2312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [00:58<00:00, 7972.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Loss: 0.2149, Train: 0.9739, Val: 0.9559, test: 0.9539\n",
      "Training Time: 0:09:46.712217 s\n",
      "Using num cluster: 1500\n",
      "Partition Time: 1.5299558639526367 s\n",
      "Epoch: 01, Loss: 1.1529\n",
      "Epoch: 02, Loss: 0.4863\n",
      "Epoch: 03, Loss: 0.3942\n",
      "Epoch: 04, Loss: 0.3567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [01:00<00:00, 7716.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Loss: 0.3439, Train: 0.9559, Val: 0.9524, test: 0.9513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Loss: 0.3230\n",
      "Epoch: 07, Loss: 0.3062\n",
      "Epoch: 08, Loss: 0.3013\n",
      "Epoch: 09, Loss: 0.3049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [01:00<00:00, 7741.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.2984, Train: 0.9609, Val: 0.9518, test: 0.9501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss: 0.2839\n",
      "Epoch: 12, Loss: 0.2775\n",
      "Epoch: 13, Loss: 0.2720\n",
      "Epoch: 14, Loss: 0.2701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [01:01<00:00, 7567.86it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss: 0.2634, Train: 0.9633, Val: 0.9513, test: 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Loss: 0.2851\n",
      "Epoch: 17, Loss: 0.2721\n",
      "Epoch: 18, Loss: 0.2635\n",
      "Epoch: 19, Loss: 0.2489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [01:00<00:00, 7680.40it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Loss: 0.2617, Train: 0.9645, Val: 0.9495, test: 0.9494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Loss: 0.2517\n",
      "Epoch: 22, Loss: 0.2424\n",
      "Epoch: 23, Loss: 0.2411\n",
      "Epoch: 24, Loss: 0.2370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [01:00<00:00, 7762.74it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Loss: 0.2379, Train: 0.9702, Val: 0.9521, test: 0.9517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Loss: 0.2414\n",
      "Epoch: 27, Loss: 0.2358\n",
      "Epoch: 28, Loss: 0.2325\n",
      "Epoch: 29, Loss: 0.2406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 465930/465930 [01:00<00:00, 7753.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Loss: 0.2327, Train: 0.9633, Val: 0.9450, test: 0.9433\n",
      "Training Time: 0:10:01.358439 s\n",
      "Using num cluster: 2000\n",
      "Computing METIS partitioning...\n",
      "Done!\n",
      "Partition Time: 298.0074031352997 s\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-655da094f6f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mstart_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-655da094f6f8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    912\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mchild_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.datasets import Reddit\n",
    "from torch_geometric.data import ClusterData, ClusterLoader, NeighborSampler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Net, self).__init__()\n",
    "        self.convs = ModuleList(\n",
    "            [SAGEConv(in_channels, 128),\n",
    "             SAGEConv(128, out_channels)])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != len(self.convs) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def inference(self, x_all):\n",
    "        pbar = tqdm(total=x_all.size(0) * len(self.convs))\n",
    "        pbar.set_description('Evaluating')\n",
    "\n",
    "        # Compute representations of nodes layer by layer, using *all*\n",
    "        # available edges. This leads to faster computation in contrast to\n",
    "        # immediately computing the final representations of each batch.\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            xs = []\n",
    "            for batch_size, n_id, adj in subgraph_loader:\n",
    "                edge_index, _, size = adj.to(device)\n",
    "                x = x_all[n_id].to(device)\n",
    "                x_target = x[:size[1]]\n",
    "                x = conv((x, x_target), edge_index)\n",
    "                if i != len(self.convs) - 1:\n",
    "                    x = F.relu(x)\n",
    "                xs.append(x.cpu())\n",
    "\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        return x_all\n",
    "\n",
    "    \n",
    "    \n",
    "class ClusterGCNNet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,hidden_dim= 128,num_layers=4):\n",
    "        super(ClusterGCNNet, self).__init__()\n",
    "        # GraphSAGE layer\n",
    "        # 这里参考了 原paper里面的4-layer的设定 + 128 hidden units\n",
    "        layer_ls=[SAGEConv(in_channels, hidden_dim)]\n",
    "        if num_layers <=2:\n",
    "            layer_ls += [SAGEConv(hidden_dim, hidden_dim) for i in range(num_layers-2)]\n",
    "        layer_ls.append(SAGEConv(hidden_dim, out_channels))\n",
    "        self.convs = ModuleList(layer_ls)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != len(self.convs) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def inference(self, x_all):\n",
    "        pbar = tqdm(total=x_all.size(0) * len(self.convs))\n",
    "        pbar.set_description('Evaluating')\n",
    "\n",
    "        # Compute representations of nodes layer by layer, using *all*\n",
    "        # available edges. This leads to faster computation in contrast to\n",
    "        # immediately computing the final representations of each batch.\n",
    "        \n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            xs = []\n",
    "            for batch_size, n_id, adj in subgraph_loader:\n",
    "                edge_index, _, size = adj.to(device)\n",
    "                x = x_all[n_id].to(device)\n",
    "                x_target = x[:size[1]]\n",
    "                x = conv((x, x_target), edge_index)\n",
    "                if i != len(self.convs) - 1:\n",
    "                    x = F.relu(x)\n",
    "                xs.append(x.cpu())\n",
    "\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        return x_all\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_nodes = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = F.nll_loss(out[batch.train_mask], batch.y[batch.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        nodes = batch.train_mask.sum().item()\n",
    "        total_loss += loss.item() * nodes\n",
    "        total_nodes += nodes\n",
    "\n",
    "    return total_loss / total_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():  # Inference should be performed on the full graph.\n",
    "    model.eval()\n",
    "\n",
    "    out = model.inference(data.x)\n",
    "    y_pred = out.argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = y_pred[mask].eq(data.y[mask]).sum().item()\n",
    "        accs.append(correct / mask.sum().item())\n",
    "    return accs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = Reddit('./data/Reddit')\n",
    "data = dataset[0]\n",
    "num_clusters = [500, 1000,1500, 2000]\n",
    "result = {\"num_cluster\":[],\"partition_t\":[],\"train_t\":[],\"train_acc\":[] ,\"val_acc\":[],\"test_acc\":[]}\n",
    "for num_part in num_clusters:\n",
    "    print(f\"Using num cluster: {num_part}\")\n",
    "    \n",
    "    start_t = time.time()\n",
    "    cluster_data = ClusterData(data, num_parts=num_part, recursive=False,\n",
    "                               save_dir=dataset.processed_dir)\n",
    "    end_t = time.time()\n",
    "    partition_t =end_t - start_t\n",
    "    print(f\"Partition Time: {partition_t} s\")\n",
    "    train_loader = ClusterLoader(cluster_data, batch_size=20, shuffle=True,\n",
    "                                 num_workers=12)\n",
    "\n",
    "    subgraph_loader = NeighborSampler(data.edge_index, sizes=[-1], batch_size=1024,\n",
    "                                      shuffle=False, num_workers=12)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = ClusterGCNNet(dataset.num_features, dataset.num_classes,hidden_dim= 128,num_layers=2).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "    start_t = time.time()\n",
    "    for epoch in range(1, 31):\n",
    "        loss = train()\n",
    "        if epoch % 5 == 0:\n",
    "            train_acc, val_acc, test_acc = test()\n",
    "            print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, '\n",
    "                  f'Val: {val_acc:.4f}, test: {test_acc:.4f}')\n",
    "        else:\n",
    "            print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "    end_t = time.time()\n",
    "    train_t =end_t - start_t\n",
    "    train_t = datetime.timedelta(seconds=train_t)\n",
    "    print(f\"Training Time: {train_t} s\")\n",
    "    result[\"num_cluster\"].append(num_part)\n",
    "    result['partition_t'].append(partition_t)\n",
    "    result['train_t'].append(train_t)\n",
    "    result[\"train_acc\"].append(train_acc) \n",
    "    result[\"val_acc\"].append(val_acc)\n",
    "    result[\"test_acc\"].append(test_acc)\n",
    "    \n",
    "df_result= pd.DataFrame(result)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Result\n",
    "这里因为训练时我尝试了不同的cluster的数目，但是都是试了3种不同cluster数目之后就内存溢出，所以这里我尝试跑了2次，分别对比500,1000,1500以及1000,1500,2000两种情况。可以看到随着cluster数目的增多 test accuracy的的变化是先大后小，而且变化的幅度不大一般都在1%左右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_cluster</th>\n",
       "      <th>partition_t</th>\n",
       "      <th>train_t</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>1.712546</td>\n",
       "      <td>0 days 00:09:43.498993</td>\n",
       "      <td>0.973591</td>\n",
       "      <td>0.956695</td>\n",
       "      <td>0.955999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>3.144444</td>\n",
       "      <td>0 days 00:09:46.712217</td>\n",
       "      <td>0.973949</td>\n",
       "      <td>0.955898</td>\n",
       "      <td>0.953916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1500</td>\n",
       "      <td>1.529956</td>\n",
       "      <td>0 days 00:10:01.358439</td>\n",
       "      <td>0.963299</td>\n",
       "      <td>0.945030</td>\n",
       "      <td>0.943306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_cluster  partition_t                train_t  train_acc   val_acc  \\\n",
       "0          500     1.712546 0 days 00:09:43.498993   0.973591  0.956695   \n",
       "1         1000     3.144444 0 days 00:09:46.712217   0.973949  0.955898   \n",
       "2         1500     1.529956 0 days 00:10:01.358439   0.963299  0.945030   \n",
       "\n",
       "   test_acc  \n",
       "0  0.955999  \n",
       "1  0.953916  \n",
       "2  0.943306  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = pd.DataFrame(result)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_cluster</th>\n",
       "      <th>partition_t</th>\n",
       "      <th>train_t</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>2.519914</td>\n",
       "      <td>0 days 00:04:56.476637</td>\n",
       "      <td>0.966519</td>\n",
       "      <td>0.948219</td>\n",
       "      <td>0.947866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1500</td>\n",
       "      <td>3.753822</td>\n",
       "      <td>0 days 00:06:28.267962</td>\n",
       "      <td>0.971231</td>\n",
       "      <td>0.953674</td>\n",
       "      <td>0.952803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>2.336999</td>\n",
       "      <td>0 days 00:07:26.611871</td>\n",
       "      <td>0.971479</td>\n",
       "      <td>0.951534</td>\n",
       "      <td>0.950075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_cluster  partition_t                train_t  train_acc   val_acc  \\\n",
       "0         2000     2.519914 0 days 00:04:56.476637   0.966519  0.948219   \n",
       "1         1500     3.753822 0 days 00:06:28.267962   0.971231  0.953674   \n",
       "2         1000     2.336999 0 days 00:07:26.611871   0.971479  0.951534   \n",
       "\n",
       "   test_acc  \n",
       "0  0.947866  \n",
       "1  0.952803  \n",
       "2  0.950075  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Conclusion and Take-away\n",
    "+ ClusterGCN的成果\n",
    "    - 对不同的batch，graph partition的方法进行研究\n",
    "    - 通过batch的设计和Clustering partition(METIS) 对GCN算法的Time, Space Complexity 在大型图数据里面有很大的提升(解决了neighborhood expansion problem问题)\n",
    "    - 相对于VR-GCN， 训练时间随着DNN 层数变多而增加的幅度不大。 Cluster-GCN的训练时间随着层数增多几乎是线性的。\n",
    "    - 能够用于训练大型embedding的特征\n",
    "  \n",
    "+ Note\n",
    "    - Vanilla 在CS里面的含义\n",
    "       vanilla is the term used to refer when computer software and sometimes also other computing-related systems like computer hardware or algorithms are not customized from their original form\n",
    "\n",
    "## 5. Reference\n",
    "[1] Datawhale: https://github.com/datawhalechina/team-learning-nlp/blob/master/GNN/Markdown%E7%89%88%E6%9C%AC/7-%E8%B6%85%E5%A4%A7%E5%9B%BE%E4%B8%8A%E7%9A%84%E8%8A%82%E7%82%B9%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0.md\n",
    "[2] Cluster-GCN 原文: https://arxiv.org/pdf/1905.07953.pdf\n",
    "\n",
    "[3] torch_geometric source code 参考: https://github.com/rusty1s/pytorch_geometric/blob/master/examples/cluster_gcn_reddit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv_v2",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
